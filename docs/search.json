[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TensorFlow 2 for Deep Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "notebooks/01a-basic-regression.html",
    "href": "notebooks/01a-basic-regression.html",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "",
    "text": "2 Setup\nImportant libraries are loaded, namely tensorflow, numpy to manipulate data, pandas to deal with table, and seaborn to create visualisation. As part of the setup, I also clean the TF2 environment with clear_session function and set seed for random number generation using set_random_seed function.\nFor the visualisation, I attempted to use the seaborn.objects interface. The reason for this is that I am familiar with the ggplot2 package in R when conducting data analysis, and I found that there is some similarity in both approach of creating a plot. For those who aren’t familiar with ggplot2 package, it employs the concept of layered grammar of graphics allowing you to describe any plots in a more structured way which result in more convenient and consistent code. You can see the documentations for the seaborn.objects interface here and the ggplot2 package here.\nFor the sample problem, I generated a 1000 observations of random number with two independent variables \\(x_0\\) and \\(x_1\\). The target for prediction is calculated using simple formula below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\nFor evaluation of model, I split the data, 80% for training and 20% for testing.\nIt is well known that deep learning models are good for high dimensional and complex data. To illustrate the capability of a model in dealing with that type of data, I slightly modified the problem by squaring x_1, giving a non-linear property to the data. The final formula is presented below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1^2 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\nUsing the same approach as above might not give you the best result as you can see in the graphs below. Both MSE and MAE can be significantly higher compared to the values from the previous problem.\nAs the name suggests, Deep Learning techniques leverages several intermediate representation of the data before finally decide what value to assign for any given input. This supports finding complex patterns that are usually inherent in real world data.\nThe previous model is modified simply by adding more Dense layers and increasing the number of the units. The activation function in a model is crucial for capturing non-linearity. The relu activation function is a function that gives either a positive value or zero which is suprisingly effective for balancing the trade-offs between finding non-linear pattern and efficient computation. As each subsequent layer can determine the number of parameters required through inferring the number of units from the previous layer, input_shape is only defined for the first layer.\nAs can be seen in the plots below and the values of MSE and MAE, the ‘deeper’ version of the model could better capture the inherent trend of the dataset leading to more superior model than the previous one.\nIn this post, I demonstrate how to leverage a small subset of TensorFlow 2 capabilities to deal with artificial datasets. Even though here only includes problems with structured data with well defined problems and boundaries, Deep Learning model in essence allows anyone to do Machine Learning for highly unstructured data such as images and texts."
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#defining-the-model",
    "href": "notebooks/01a-basic-regression.html#defining-the-model",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "4.1 Defining the model",
    "text": "4.1 Defining the model\nDefining a TF2 model can be accomplished easily by calling the Sequential method, followed by adding any types and number of layers. As the problem is very straightforward, tackling this should be relatively easy. For this reason, the model I defined here is very simple with only one Dense layer with one perceptron unit. In addition, we need to define the input_shape so that the model can determine the number of parameters it requires to predict all inputs. The summary method allows you to see the architecture of the model.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 3\nTrainable params: 3\nNon-trainable params: 0\n_________________________________________________________________\n\n\nBefore training the model, you are required to choose the optimizer, loss function, and metrics, and compile those into the model. Here, I decided to use Stochastic Gradient Descent algorithm for optimizing the model (i.e., updating neural network parameters), Mean Squared Error (MSE) for determining how far the prediciton of the current model with actual values, and Mean Absolute Error (MAE) as a metric to evaluate the model.\n\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#training-the-model",
    "href": "notebooks/01a-basic-regression.html#training-the-model",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "4.2 Training the model",
    "text": "4.2 Training the model\nTraining can be done using fit method. The process is done after 100 epochs or cycles of the model updating its parameters based on input data. The verbose parameter that equals 0 means that the training will not print any information.\n\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\nThe fit method returns History object, which provides you the performance of the model during training. The history.history contains all loss and metric scores for all training epochs, and you can extract this information to evalate your model. I performed some manipulation basically to have a certain format of data (the long version, you might want to refer to tidy data by Hadley Wickham).\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\n    \"index\":\"epoch\", \n    \"loss\": \"Mean Squared Error\", \n    \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(\n    data, \n    id_vars = \"epoch\", \n    value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n    var_name = \"metric\",\n    value_name = \"value\"\n)\n\ndata.sort_values(by = \"epoch\").head()\n\n\n\n\n\n  \n    \n      \n      epoch\n      metric\n      value\n    \n  \n  \n    \n      0\n      0\n      Mean Squared Error\n      5.752564\n    \n    \n      100\n      0\n      Mean Absolute Error\n      1.881032\n    \n    \n      1\n      1\n      Mean Squared Error\n      2.836762\n    \n    \n      101\n      1\n      Mean Absolute Error\n      1.315763\n    \n    \n      2\n      2\n      Mean Squared Error\n      1.707698\n    \n  \n\n\n\n\nWe can then visualise how the model perform throughout the training.\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#evaluating-the-model",
    "href": "notebooks/01a-basic-regression.html#evaluating-the-model",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "4.3 Evaluating the model",
    "text": "4.3 Evaluating the model\nFinally, we can check the model’s performance on the test dataset. The evaluate method allows the users to see how the model perform when predicting unseen data. The model seems to do good in predicting the actual output based on the MSE and MAE.\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 0.90\nMean Absolute Error: 0.77"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html",
    "href": "notebooks/01b-basic-classification.html",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "",
    "text": "3 Generate Random Data\nFor this use case, I genereated two classes of data based on multivariate normal distribution with specified means and covariance matrices. Both values are determined arbitrarily.\nAs there are only two variables within the data, making sense of it is easier as we only requires a scatter plot to see how data is dispersed along x and y axes. As you can see from the figure below, there is an area where points from class 1 and class 2 overlap.\nTo help slicing two python variables with the same length (X and y), I created a vector of data indices where the order is shuffled. This then server as a reference to determine which points belong to which datasets (training, validation, or testing).\nI split the data into train and test datasets (80% and 20%), before splitting the train dataset further for hyperparameter tuning into partial train and validation (80% and 20%).\nHyperparameter optimization or tuning can be applied to any parameters controlling the behaviours of the machine learning algorithm which are not learned during training. In doing so, we need to separate the test data and leverage two subsets of training data instead. Otherwise, there might be any leak of information from the ‘unseen data’ which might alter the result of the trained algorithm giving it the capability to perform better on the test dataset. This opposes the idea of ML model that should be able to do well given unknown input, which, in this case is represented as test dataset.\nThe hyperparameter to be tuned is the simple one, in this case number of epochs.The process includes training a network with simplifed architecture, then analyses the performance of the network throughout the training. The optimal number of epochs is decided based on how accuracy and loss values moves throughout time.\nThe actual workflow for creating the model, compiling its optimizer, loss function, and metrics, and fitting it to the data is similar to what you can see from the previous demo. The difference here is that I did not use model.add method to put a layer in the model. Instead, I gave a list of several Dense layers as an argument when instantiating a Sequential model. In addition, the number of units in each layer is a reduced one (we will increase it when training with full train data). I also set the learning rate for the SGD optimizer into 0.005.\nThe model is fitted using the partial_X_train and partial_y_train with a set of validation data. By using validation data, we might observe how the performance of the model throughout training.\nBelow, we can see the values of training and validation accuracy and loss given a certain training epoch. Because of the values for the validation seems to resemble training values, it can be inferred that the model does not overfit. Overfitting may cause the training accuracy to be significantly higher than validation accuracy and training loss to be significantly lower than validation loss.\nAfter observing how the simplified model performs, we were able to decide at which epoch we want to stop training our model. In this case, we selected 175 as the subsequent epochs does not give improvement to the model (the loss seemed to stop decreasing). We then could fit our model with full training data and increase the number of units for each Dense layer.\nNext, we see how the model classifies each data point from the graph below.\nWe could also evaluate the performance on the test dataset. The model can reach more than 80% accuracy.\nIn this post, we continue our demonstration of TensorFlow 2 with classification problems. The model successfully achieve a decent accuracy score for this simple case. Additionally, we have touched the concept of hyperparameter tuning which is essential for doing machine learning."
  },
  {
    "objectID": "notebooks/02a-mnist.html",
    "href": "notebooks/02a-mnist.html",
    "title": "3  MNIST",
    "section": "",
    "text": "# import libraries\nimport numpy as np\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# transform image data\ntrain_images = train_images.reshape((60000, 28 * 28)) / 255\ntest_images = test_images.reshape((10000, 28 * 28)) / 255\n\ntrain_images.shape, train_labels.shape, test_images.shape, test_labels.shape\n\n\ndef explore(train_images, \n            train_labels, \n            test_images, \n            test_labels,\n            label_count,\n            neuron_count,\n            learning_rate,\n            momentum):\n\n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Dense(neuron_count, activation = \"relu\", input_shape = (28 * 28,)))\n    model.add(layers.Dense(label_count, activation = \"softmax\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    model.compile(optimizer = optimizer,\n                  loss = \"categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n\n    # train ann model\n    history = model.fit(train_images, train_labels, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n\n    return history, test_loss, test_acc\n\n\n# set hyperparameters\nlearning_rates = np.logspace(-1, -4, 5)\nmomentums = np.logspace(-1, -4, 5)\nneuron_counts = 2 ** np.arange(7, 12)\n\nhyparameters_list = []\nfor learning_rate in learning_rates:\n    for momentum in momentums:\n        for neuron_count in neuron_counts:\n            hyparameters_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"neuron_count\": neuron_count\n            })\n\n\noutput = []\nfor hyparameters in hyparameters_list:\n    history, test_loss, test_acc = explore(\n        train_images,\n        train_labels,\n        test_images,\n        test_labels,\n        label_count = 10,\n        learning_rate = hyparameters[\"learning_rate\"],\n        momentum = hyparameters[\"momentum\"],\n        neuron_count = hyparameters[\"neuron_count\"]\n    )\n    \n    output.append({\n        \"history\": history,\n        \"test_loss\": test_loss, \n        \"test_acc\": test_acc,\n        \"hyperparameters\": hyparameters\n    })\n    backend.clear_session()\n\n    print(f\"A model is trained with hyperparameters of {hyparameters}\")"
  },
  {
    "objectID": "notebooks/02b-imdb.html",
    "href": "notebooks/02b-imdb.html",
    "title": "4  IMDB",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\n# preprocess\nX_train = np.zeros(shape = (len(train_data), num_words), dtype = float)\nX_test = np.zeros(shape = (len(test_data), num_words), dtype = float)\n\nfor i, seq in enumerate(train_data):\n    X_train[i, seq] = 1.\n\nfor i, seq in enumerate(test_data):\n    X_test[i, seq] = 1.\n\ny_train = train_labels.astype(float)\ny_test = test_labels.astype(float)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\n\npartial_X_train = X_train[:12500]\npartial_y_train = y_train[:12500]\nX_val = X_train[12500:]\ny_val = y_train[12500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            n_units,\n            n_layers,\n            activation,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    for i in range(n_layers):\n        model.add(layers.Dense(n_units, activation = activation))\n    model.add(layers.Dense(1, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n\n    # train ann model\n    model.build(input_shape = (10000,))\n    model.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nn_unit_list         = [32, 64]\nn_hidden_layer_list = [1, 3]\nactivation_list     = [\"relu\", \"tanh\"]\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for n_units in n_unit_list:\n            for n_layers in n_hidden_layer_list:\n                for activation in activation_list:\n                    param_list.append({\n                        \"learning_rate\": learning_rate,\n                        \"momentum\": momentum,\n                        \"n_units\": n_units,\n                        \"n_layers\": n_layers,\n                        \"activation\": activation\n                    })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        n_units = params[\"n_units\"],\n        n_hidden_layer = params[\"n_hidden_layer\"],\n        activation = params[\"activation\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02c-reuters.html",
    "href": "notebooks/02c-reuters.html",
    "title": "5  Reuters",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import reuters\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels,), (test_data, test_labels) = reuters.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\nseq_len = 300 # the avg is 145.54\n\nX_train = [seq[:seq_len] for seq in train_data]\nX_train = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_train]\nX_train = np.array(X_train).astype(int)\n\ny_train = to_categorical(train_labels)\n\nX_test = [seq[:seq_len] for seq in test_data]\nX_test = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_test]\nX_test = np.array(X_test).astype(int)\n\ny_test = to_categorical(test_labels)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\npartial_X_train = X_train[:4500]\npartial_y_train = y_train[:4500]\nX_val = X_train[4500:]\ny_val = y_train[4500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            embedding_dim,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Embedding(num_words, embedding_dim, input_length = seq_len))\n    model.add(layers.Dense(64, activation = \"relu\"))\n    model.add(layers.Dense(46, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    # train ann model\n    model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nembedding_dim_list  = 2 ** np.arange(3, 7)\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for embedding_dim in embedding_dim_list:\n            param_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"embedding_dim\": embedding_dim\n            })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        embedding_dim = params[\"embedding_dim\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02d-boston.html",
    "href": "notebooks/02d-boston.html",
    "title": "6  Boston Housing",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.base import clone\nfrom tensorflow.keras.datasets import boston_housing\nfrom tensorflow.keras import models, layers, backend\n\n\n# load dataset\n(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\n# rescale and shift data based on training set\ntransform_mean = np.mean(X_train)\ntransform_std  = np.std(X_train, ddof = 1)\n\nX_train -= transform_mean\nX_train /= transform_std\n\nX_test -= transform_mean\nX_test /= transform_std\n\n\nmodel_nn = models.Sequential()\nmodel_nn.add(layers.Dense(128, activation = \"relu\", input_shape = (13,)))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(1))\nmodel_nn.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\n\ninitial_weight_nn = model_nn.get_weights()\n\n\nmodel_nn_reg = models.Sequential()\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", input_shape = (13,)))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(1))\n\nmodel_nn_reg.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\ninitial_weight_nn_reg = model_nn_reg.get_weights()\n\n\nlm_base = LinearRegression()\n\n\nindices = np.arange(len(X_train))\nnp.random.seed(123)\nnp.random.shuffle(indices)\n\nk_fold = 5\nsample_size = np.ceil(len(X_train) / k_fold).astype(int)\n\n\nmse_nn, mse_nn_reg, mse_lm = [], [], []\nmae_nn, mae_nn_reg, mae_lm = [], [], []\n\nfor i in range(k_fold):\n    # configure model with exact parameters\n    model_lm = clone(lm_base)\n    model_nn.set_weights(initial_weight_nn)\n    model_nn_reg.set_weights(initial_weight_nn_reg)\n\n    # split into partial_train and validation\n    id_start, id_end = i * sample_size, (i+1) * sample_size\n\n    mask_train = np.concatenate((indices[:id_start], indices[id_end:]))\n    mask_val   = indices[id_start:id_end]\n\n    X_val = X_train[mask_val]\n    y_val = y_train[mask_val]\n    partial_X_train = X_train[mask_train]\n    partial_y_train = y_train[mask_train]\n\n    # fit and predict\n    model_lm.fit(partial_X_train, partial_y_train)\n    model_nn.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n    model_nn_reg.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n\n    y_pred_lm = model_lm.predict(X_val)\n    y_pred_nn = model_nn.predict(X_val, verbose = 0)\n    y_pred_nn_reg = model_nn_reg.predict(X_val, verbose = 0)\n\n    # save results\n    mse_nn.append(mean_squared_error(y_val, y_pred_nn))\n    mse_nn_reg.append(mean_squared_error(y_val, y_pred_nn_reg))\n    mse_lm.append(mean_squared_error(y_val, y_pred_lm))\n\n    mae_nn.append(mean_absolute_error(y_val, y_pred_nn))\n    mae_nn_reg.append(mean_absolute_error(y_val, y_pred_nn_reg))\n    mae_lm.append(mean_absolute_error(y_val, y_pred_lm))\n\n\nprint(f\"Avg MSE of Neral Network : {np.mean(mse_nn):.2f}\")\nprint(f\"Avg MSE of NN Regulaized : {np.mean(mse_nn_reg):.2f}\")\nprint(f\"Avg MSE of Linear Model  : {np.mean(mse_lm):.2f}\")\n\n\nprint(f\"Avg MAE of Neral Network : {np.mean(mae_nn):.2f}\")\nprint(f\"Avg MAE of NN Regulaized : {np.mean(mae_nn_reg):.2f}\")\nprint(f\"Avg MAE of Linear Model  : {np.mean(mae_lm):.2f}\")"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#setup",
    "href": "notebooks/01b-basic-classification.html#setup",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nThe setup includes importing important libraries (tensorflow, numpy, and matplotlib.pyplot), freeing memory from old models/layers (if any) and setting the seed for random number generator.\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntf.keras.backend.clear_session()\ntf.keras.utils.set_random_seed(123)"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#generate-random-data",
    "href": "notebooks/01b-basic-classification.html#generate-random-data",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.2 Generate Random Data",
    "text": "2.2 Generate Random Data\nFor this use case, I genereated two classes of data based on multivariate normal distribution with specified means and covariance matrices. Both values are determined arbitrarily.\n\n# random data generation\nSAMPLE_SIZE = 1500\n\nclass_1 = np.random.multivariate_normal(\n    mean = [0, 2],\n    cov  = [[1, 0.1], [0.1, 1]],\n    size = SAMPLE_SIZE\n)\n\nclass_2 = np.random.multivariate_normal(\n    mean = [2, 0],\n    cov  = [[1, 0.1], [0.1, 1]],\n    size = SAMPLE_SIZE\n)\n\n# append both classes\nX = np.concatenate([class_1, class_2])\nX = X.astype(\"float32\")\n\ny = np.concatenate([np.zeros((SAMPLE_SIZE, 1)), np.ones((SAMPLE_SIZE, 1))])\ny = y.astype(\"int\")\n\nX.shape, y.shape\n\n((3000, 2), (3000, 1))\n\n\nAs there are only two variables within the data, making sense of it is easier as we only requires a scatter plot to see how data is dispersed along x and y axes. As you can see from the figure below, there is an area where points from class 1 and class 2 overlap.\n\nplt.scatter(X[:, 0], X[:, 1], c = y[:, 0], alpha = .2)\nplt.show()"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#slice-the-data",
    "href": "notebooks/01b-basic-classification.html#slice-the-data",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.3 Slice the Data",
    "text": "2.3 Slice the Data\nTo help slicing two python variables with the same length (X and y), I created a vector of data indices where the order is shuffled. This then server as a reference to determine which points belong to which datasets (training, validation, or testing).\nI split the data into train and test datasets (80% and 20%), before splitting the train dataset further for hyperparameter tuning into partial train and validation (80% and 20%).\n\n# define randomized indices for splitting\nindices = np.arange(SAMPLE_SIZE * 2)\nnp.random.shuffle(indices)\n\n# split data into `train` and `test datasets`\nsplit_locaction = round(SAMPLE_SIZE * .8)\n\nX_train = X[indices[:split_locaction]]\ny_train = y[indices[:split_locaction]]\n\nX_test = X[indices[split_locaction:]]\ny_test = y[indices[split_locaction:]]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((1200, 2), (1200, 1), (1800, 2), (1800, 1))\n\n\n\n# split train data into `partial` and `validation` for hyperparameter tuning\nsplit_locaction = round(len(X_train) * .8)\n\npartial_X_train = X_train[:split_locaction]\npartial_y_train = y_train[:split_locaction]\n\nX_val = X_train[split_locaction:]\ny_val = y_train[split_locaction:]\n\npartial_X_train.shape, partial_y_train.shape, X_val.shape, y_val.shape\n\n((960, 2), (960, 1), (240, 2), (240, 1))"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#hyperparameter-optimization",
    "href": "notebooks/01b-basic-classification.html#hyperparameter-optimization",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.4 Hyperparameter Optimization",
    "text": "2.4 Hyperparameter Optimization\nHyperparameter optimization or tuning can be applied to any parameters controlling the behaviours of the machine learning algorithm which are not learned during training. In doing so, we need to separate the test data and leverage two subsets of training data instead. Otherwise, there might be any leak of information from the ‘unseen data’ which might alter the result of the trained algorithm giving it the capability to perform better on the test dataset. This opposes the idea of ML model that should be able to do well given unknown input, which, in this case is represented as test dataset.\nThe hyperparameter to be tuned is the simple one, in this case number of epochs.The process includes training a network with simplifed architecture, then analyses the performance of the network throughout the training. The optimal number of epochs is decided based on how accuracy and loss values moves throughout time.\nThe actual workflow for creating the model, compiling its optimizer, loss function, and metrics, and fitting it to the data is similar to what you can see from the previous demo. The difference here is that I did not use model.add method to put a layer in the model. Instead, I gave a list of several Dense layers as an argument when instantiating a Sequential model. In addition, the number of units in each layer is a reduced one (we will increase it when training with full train data). I also set the learning rate for the SGD optimizer into 0.005.\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(8, input_shape = (2,), activation = \"relu\"),\n    tf.keras.layers.Dense(8, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.005), \n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nhistory = model.fit(partial_X_train, \n                    partial_y_train, \n                    validation_data=(X_val, y_val), \n                    epochs = 1000, \n                    verbose = 0)\n\nThe model is fitted using the partial_X_train and partial_y_train with a set of validation data. By using validation data, we might observe how the performance of the model throughout training.\nBelow, we can see the values of training and validation accuracy and loss given a certain training epoch. Because of the values for the validation seems to resemble training values, it can be inferred that the model does not overfit. Overfitting may cause the training accuracy to be significantly higher than validation accuracy and training loss to be significantly lower than validation loss.\n\n# Plot the training results\naccuracy     = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nepochs       = range(len(accuracy))\n\nplt.plot(epochs, accuracy, 'r', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.ylim(ymin=0)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the training results\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.ylim(ymin=0)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#fitting-with-full-training-data",
    "href": "notebooks/01b-basic-classification.html#fitting-with-full-training-data",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.5 Fitting with Full Training Data",
    "text": "2.5 Fitting with Full Training Data\nAfter observing how the simplified model performs, we were able to decide at which epoch we want to stop training our model. In this case, we selected 175 as the subsequent epochs does not give improvement to the model (the loss seemed to stop decreasing). We then could fit our model with full training data and increase the number of units for each Dense layer.\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, input_shape = (2,), activation = \"relu\"),\n    tf.keras.layers.Dense(64, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.005), \n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 175, verbose = 0)\n\nNext, we see how the model classifies each data point from the graph below.\n\ny_pred = model.predict(X_test, verbose = 0)\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred[:, 0] > .5, alpha = .3)\nplt.show()\n\n\n\n\n\n\n\n\nWe could also evaluate the performance on the test dataset. The model can reach more than 80% accuracy.\n\nloss, accuracy = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Loss    : {loss:.3f}\")\nprint(f\"Accuracy: {accuracy:.3f}\")\n\nLoss    : 0.176\nAccuracy: 0.926"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html#conclusion",
    "href": "notebooks/01b-basic-classification.html#conclusion",
    "title": "2  TensorFlow 2 for Classification Problems",
    "section": "2.6 Conclusion",
    "text": "2.6 Conclusion\nIn this post, we continue our demonstration of TensorFlow 2 with classification problems. The model successfully achieve a decent accuracy score for this simple case. Additionally, we have touched the concept of hyperparameter tuning which is essential for doing machine learning."
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#setup",
    "href": "notebooks/01a-basic-regression.html#setup",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.1 Setup",
    "text": "1.1 Setup\nImportant libraries are loaded, namely tensorflow, numpy to manipulate data, pandas to deal with table, and seaborn to create visualisation. As part of the setup, I also clean the TF2 environment with clear_session function and set seed for random number generation using set_random_seed function.\nFor the visualisation, I attempted to use the seaborn.objects interface. The reason for this is that I am familiar with the ggplot2 package in R when conducting data analysis, and I found that there is some similarity in both approach of creating a plot. For those who aren’t familiar with ggplot2 package, it employs the concept of layered grammar of graphics allowing you to describe any plots in a more structured way which result in more convenient and consistent code. You can see the documentations for the seaborn.objects interface here and the ggplot2 package here.\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport seaborn.objects as so\n\n\ntf.keras.backend.clear_session()\ntf.keras.utils.set_random_seed(123)"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#generate-random-data",
    "href": "notebooks/01a-basic-regression.html#generate-random-data",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.2 Generate Random Data",
    "text": "1.2 Generate Random Data\nFor the sample problem, I generated a 1000 observations of random number with two independent variables \\(x_0\\) and \\(x_1\\). The target for prediction is calculated using simple formula below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\n\nX = np.random.normal(size = (1000, 2))\ny = 0.2 * X[:, 0] + 2.8 * X[:, 1] + np.random.normal(size = (1000,))\n\nFor evaluation of model, I split the data, 80% for training and 20% for testing.\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, \n\n((800, 2), (800,), (200, 2), (200,))"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#a-very-simple-tf2-model",
    "href": "notebooks/01a-basic-regression.html#a-very-simple-tf2-model",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.3 A Very Simple TF2 Model",
    "text": "1.3 A Very Simple TF2 Model\n\n1.3.1 Defining the model\nDefining a TF2 model can be accomplished easily by calling the Sequential method, followed by adding any types and number of layers. As the problem is very straightforward, tackling this should be relatively easy. For this reason, the model I defined here is very simple with only one Dense layer with one perceptron unit. In addition, we need to define the input_shape so that the model can determine the number of parameters it requires to predict all inputs. The summary method allows you to see the architecture of the model.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 3\nTrainable params: 3\nNon-trainable params: 0\n_________________________________________________________________\n\n\nBefore training the model, you are required to choose the optimizer, loss function, and metrics, and compile those into the model. Here, I decided to use Stochastic Gradient Descent algorithm for optimizing the model (i.e., updating neural network parameters), Mean Squared Error (MSE) for determining how far the prediciton of the current model with actual values, and Mean Absolute Error (MAE) as a metric to evaluate the model.\n\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\n\n\n\n1.3.2 Training the model\nTraining can be done using fit method. The process is done after 100 epochs or cycles of the model updating its parameters based on input data. The verbose parameter that equals 0 means that the training will not print any information.\n\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\nThe fit method returns History object, which provides you the performance of the model during training. The history.history contains all loss and metric scores for all training epochs, and you can extract this information to evalate your model. I performed some manipulation basically to have a certain format of data (the long version, you might want to refer to tidy data by Hadley Wickham).\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\n    \"index\":\"epoch\", \n    \"loss\": \"Mean Squared Error\", \n    \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(\n    data, \n    id_vars = \"epoch\", \n    value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n    var_name = \"metric\",\n    value_name = \"value\"\n)\n\ndata.sort_values(by = \"epoch\").head()\n\n\n\n\n\n  \n    \n      \n      epoch\n      metric\n      value\n    \n  \n  \n    \n      0\n      0\n      Mean Squared Error\n      5.752564\n    \n    \n      100\n      0\n      Mean Absolute Error\n      1.881032\n    \n    \n      1\n      1\n      Mean Squared Error\n      2.836762\n    \n    \n      101\n      1\n      Mean Absolute Error\n      1.315763\n    \n    \n      2\n      2\n      Mean Squared Error\n      1.707698\n    \n  \n\n\n\n\nWe can then visualise how the model perform throughout the training.\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\n\n1.3.3 Evaluating the model\nFinally, we can check the model’s performance on the test dataset. The evaluate method allows the users to see how the model perform when predicting unseen data. The model seems to do good in predicting the actual output based on the MSE and MAE.\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 0.90\nMean Absolute Error: 0.77"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#dealing-with-non-linearity",
    "href": "notebooks/01a-basic-regression.html#dealing-with-non-linearity",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.4 Dealing with Non-linearity",
    "text": "1.4 Dealing with Non-linearity\nIt is well known that deep learning models are good for high dimensional and complex data. To illustrate the capability of a model in dealing with that type of data, I slightly modified the problem by squaring x_1, giving a non-linear property to the data. The final formula is presented below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1^2 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\n\nX = np.random.normal(size = (1000, 2))\ny = 0.2 * X[:, 0] + 2.8 * X[:, 1] ** 2 + np.random.normal(size = (1000,))\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, \n\n((800, 2), (800,), (200, 2), (200,))\n\n\nUsing the same approach as above might not give you the best result as you can see in the graphs below. Both MSE and MAE can be significantly higher compared to the values from the previous problem.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\"index\":\"epoch\", \n                              \"loss\": \"Mean Squared Error\", \n                              \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(data, \n               id_vars = \"epoch\", \n               value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n               var_name = \"metric\",\n               value_name = \"value\")\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 14.94\nMean Absolute Error: 2.75"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#going-deeper-by-using-more-layers",
    "href": "notebooks/01a-basic-regression.html#going-deeper-by-using-more-layers",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.5 Going Deeper by Using More Layers",
    "text": "1.5 Going Deeper by Using More Layers\nAs the name suggests, Deep Learning techniques leverages several intermediate representation of the data before finally decide what value to assign for any given input. This supports finding complex patterns that are usually inherent in real world data.\nThe previous model is modified simply by adding more Dense layers and increasing the number of the units. The activation function in a model is crucial for capturing non-linearity. The relu activation function is a function that gives either a positive value or zero which is suprisingly effective for balancing the trade-offs between finding non-linear pattern and efficient computation. As each subsequent layer can determine the number of parameters required through inferring the number of units from the previous layer, input_shape is only defined for the first layer.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (2, )))\nmodel.add(tf.keras.layers.Dense(32, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\n\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 32)                96        \n                                                                 \n dense_3 (Dense)             (None, 32)                1056      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1,185\nTrainable params: 1,185\nNon-trainable params: 0\n_________________________________________________________________\n\n\nAs can be seen in the plots below and the values of MSE and MAE, the ‘deeper’ version of the model could better capture the inherent trend of the dataset leading to more superior model than the previous one.\n\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\"index\":\"epoch\", \n                              \"loss\": \"Mean Squared Error\", \n                              \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(data, \n               id_vars = \"epoch\", \n               value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n               var_name = \"metric\",\n               value_name = \"value\")\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 1.04\nMean Absolute Error: 0.82"
  },
  {
    "objectID": "notebooks/01a-basic-regression.html#conclusion",
    "href": "notebooks/01a-basic-regression.html#conclusion",
    "title": "1  TensorFlow 2 for Regression Problems",
    "section": "1.6 Conclusion",
    "text": "1.6 Conclusion\nIn this post, I demonstrate how to leverage a small subset of TensorFlow 2 capabilities to deal with artificial datasets. Even though here only includes problems with structured data with well defined problems and boundaries, Deep Learning model in essence allows anyone to do Machine Learning for highly unstructured data such as images and texts."
  }
]