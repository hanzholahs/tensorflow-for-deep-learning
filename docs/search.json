[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning with TensorFlow 2 (DRAFT)",
    "section": "",
    "text": "Welcome\nWelcome to for Deep Learning using TensorFlow 2, a book that will show you how to use TensorFlow 2 to build and train your own models. In this book, you will learn how to use TensorFlow 2 and its high-level API, Keras, to implement various deep learning techniques, such as artificial neural networks, convolutional neural networks, recurrent neural networks, and more. In addition, several approaches to deal with different types of data, such as image, text, and time series data, will be discussed. You will learn the fundamental of TensorFlow 2 for use cases in some of the most popular and challenging domains of deep learning, which are computer vision, natural language processing, and time series data.\nThis book is designed for beginners who want to learn TensorFlow 2 from scratch by practicing the techniques on practical examples and projects. Whether you are new to TensorFlow or have some experience with it, this book will help you master the essential skills and concepts you need to create effective solutions for real-world problems. Nevertheless, you are assumed to have some backgrounds in Python programming. Each chapter provides a brief introduction to the theory behind each technique, followed by a step-by-step guide on how to implement it using TensorFlow 2, Keras, and other relevant libraries.\nBy the end of this book, you will have a solid foundation in TensorFlow 2 and deep learning, and you will be able to use your skills and knowledge to create your own projects or advance your career in this exciting and fast-growing field.\n\nCollaboration\nThis book is currently under development. The source code for this website is available on GitHub. We welcome any contributions or feedback from other developers who are interested in this project. If you find any errors or typos, please feel free to open an issue or a pull request. Thank you for your support and collaboration.\n\n\nAuthors\n\nHanzholah Shobri, a Researcher at Bank Indonesia. He hold a bachelor’s degree in Industrial Engineering from Universitas Gadjah Mada, Yogyakarta. (contacts: email, github, twitter)\nTria Rahmat Mauludin, a Researcher at Bank Indonesia. He hold a bachelor’s degree in Mathematics from Insitut Teknologi Bandung, Bandung. (contacts: email, github)\n\n\n\nLicense\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nThe code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Deep learning is a branch of artificial intelligence that uses neural networks to learn from data and perform certain tasks. Many creative and innnovative solutions to human problems have been solved by deep learning techniques. Hence, the skills to use deep learning in any domain is important in the era where computational capabilty and data availability is continally increasing. This book consists of the application of several key techniques in deep learning to deal with complex data structures.\nIf you are familiar with Python and want to learn TensorFlow by solving real-world problems, this book is for you. You will learn how to use TensorFlow to build machine learning models for various domains, such as computer vision, natural language processing, and more. You will also get hands-on experience with TensorFlow through case studies and practical examples that show you how to apply the concepts and techniques in your own projects. Every relevant concept is introduced gradually, so it is recommended to read the book from the first chapter to the end.\n\nThe concepts of Artificial Intelligence, Machine Learning, and Deep Learning\nTo understand what deep learning is, we need to also look at the definition of machine learning and deep learning. In principle, deep learning is a subset of machine learning, and machine learning is a subset of artificial intelligence.\nArtificial intelligence (AI) is a broad term that refers to any system or software that can perform tasks that normally require human intelligence, such as reasoning, decision making, problem solving, natural language processing, and computer vision. Machine learning (ML) is a subset of AI that focuses on creating systems or software that can learn patterns or rules from data and improve their performance without explicit programming or human intervention.\nDeep learning (DL) is a subset of ML that uses artificial neural networks (ANNs) to model complex and nonlinear relationships between inputs and outputs. In contrast to traditional machine learning that requires manual feature engineering to extract relevant information, deep learning uses multiple intermediate representations of the data to extract features automatically from raw data.\nAn illustration by Data Iku on the relationship between AI, ML, and DL is presented below.\n\n\n\nDL is a subset of ML, and ML is a subset of AI\n\n\n\n\nAn Introduction to TensorFlow 2.0 and Keras Libraries\nTensorflow 2 and Keras are two powerful frameworks for building and deploying machine learning solutions, especially for deep learning. Tensorflow 2 is the premier open-source platform for developing, training, and deploying deep learning models at scale. Keras is the high-level API of Tensorflow 2, which provides an easy and intuitive way to design, fit, evaluate, and use deep learning models with just a few lines of code.\nTensorflow 2 and Keras have many advantages over other frameworks, such as:\n\nThey support multiple backends, devices, and languages, allowing you to run your code on CPUs, GPUs, TPUs, and even mobile devices.\nThey offer a rich set of tools and libraries for data processing, visualization, debugging, optimization, and deployment.\nThey enable you to use both the sequential and functional APIs to create simple or complex models with different architectures and layers.\nThey allow you to leverage the power of custom layers, custom training loops, custom callbacks, and custom metrics to customize your model according to your needs.\nThey integrate seamlessly with other popular libraries and frameworks, such as NumPy, SciPy, Pandas, Scikit-learn, PyTorch, and more.\n\nTensorflow 2 and Keras are not only frameworks but also communities of developers, researchers, and practitioners who share their knowledge and experience through blogs, tutorials, books, courses, forums, and events. By using Tensorflow 2 and Keras, you can join this vibrant community and learn from the best in the field.\n\n\nHow Will You Learn Deep Learning by Using This Book?\nThis book exclusively uses TensorFlow 2, as well as its high-level API or Keras, to build and train a deep learning model. Other libraries are also used for preparing the data or making visualisation. To practice with the code in this book, readers could access the source notebook or copy and paste each code manually. There are three parts of the book.\n\nPart 1: Basic Usage of TensorFlow 2. This part contains the basic workflow to build a model that is used in this book and how to implement TensorFlow 2 according to the workflow for regression and classification problems using dummy data.\nPart 2: Use Cases of TensorFlow 2. This part demonstrates several applications of some important techniques in machine learning and TensorFlow 2 to four use cases using datasets provided by TensorFlow 2 library.\nPart 3: Preparing Data for TensorFlow 2. This part goes beyond built-in dataset and shows how to handle messy, real-world data in different formats (images, texts, and time series data) into inputs for TensorFlow 2 deep learning models."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#machine-learning",
    "href": "notebooks/01a-basic-workflow.html#machine-learning",
    "title": "1  General Idea",
    "section": "1.1 Machine Learning",
    "text": "1.1 Machine Learning\nFor developers, data scientists, or researchers, machine learning techniques offer them with the capability to handle complex problems by training a mathematical model that can extract some rules or patterns within a sample and use those to generate some predictions. This usually goes beyond the traditional statistical methods that deals with a smaller amount of data.\nIn machine learning, the learning process revolves around fitting a model to a dataset that consists of input data and corresponding desired outputs, known as labels. During training, the model learns patterns and relationships within the data, allowing it to generalize and make predictions on, samples beyond the information provided in the training process, new, unseen data.\n\n1.1.1 Types of problems\nMachine Learning problems can generally be classified into three, namely supervised learning, unsupervised learning, and reinforcement learning.\n\nIn a supervised learning, the model usually is given a dataset containing sets of feature data and its label. The system is trained in order to learn a mapping rules of the inputs to the respective outputs so that it can infer the label of a new, previously-unseen observations.\nIn unsupervised learning, the model processes raw data that is neither labeled nor tagged by human. The purpose of this type of problem is to find hidden patterns or structure in the data.\nIn reinforcement learning, the model is trained to be the brain of an independent agent which interact with certain environment by performing some actions, observing the impacts of its actions in the environment, and receiving reward or penalties based on its actions.\n\nOne thing to note is that sometimes the lines between these types of problem can be blurry. For example, one can deal with data that is half unlabeled and half labeled in a semi-supervised learning which is a mixture of both supervised and unsupervised learning.\nThis book discusses several use cases of deep learning in different applications, including in natural language processing, computer vision, and time series analysis. However, each use case is supervised learning where the model is trained using labeled input data and used to predict new dataset. Specifically, all techniques learned in this book is used for either regression or classification problems. Regression is a type of problem where the model needs to predict continuous, numerical data whereas, for classification problem, the model has to infer categorical label of the data.\n\n\n1.1.2 The roles of data\nMachine learning models use data as the building blocks that enable models to learn hidden patterns and make accurate predictions. High-quality and relevant data is crucial for the success of any machine learning. The more diverse, representative, and comprehensive the data, the better the model’s ability to generalize and make accurate predictions. Data is represented in many different forms such as numbers, texts, images, and recordings.\nIt is the job of the developers to ensure the model can learn meaningful patterns and generalise to previously unseen data. This is done by carefully performing data preparation and feature engineering so that the model can be trained on high quality input. Data preparation refers to a series of steps and techniques applied to raw data before it is used for analysis or model training. It is a crucial task as real-world data often contains noise, missing values, inconsistencies, outliers, or irrelevant information that can adversely affect the performance and accuracy of machine learning models. Effective feature engineering is the process of selecting, transforming, and creating meaningful features that capture the underlying patterns and relationships in the data. Good features should have predictive power and be informative for the task at hand. The choice and quality of features significantly impact the performance of a machine learning model.\nWhen building a model, it is often that not all data is used for training and some portion of it is used to represent unseen data in the evaluation process. There are different techniques that can be applied, and the simplest way to do this is to separate between data for training and testing. The parameters of the model are updated based on how the model perform on the training dataset, while the overall performance of the model is calculated using some metrics using the testing dataset."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#deep-learning",
    "href": "notebooks/01a-basic-workflow.html#deep-learning",
    "title": "1  General Idea",
    "section": "1.2 Deep Learning",
    "text": "1.2 Deep Learning\nDeep learning is a powerful and popular branch of machine learning that uses neural networks to learn from data and perform complex tasks. Deep learning, as previously illustrated, is a part of machine learning that leverages multiple layers to learn representations of data hierarchically to make predictions on a given input data. Therefore, there are some concepts that are also applied when someone want to use deep learning models.\n\n1.2.1 Components of deep learning\n\nInspired by the structure and functioning of the human brain, deep learning aims to mimic the way neurons process and transmit information using artificial neural networks. The networks consist of interconnected nodes called neurons, which are organized into layers. There are three main layers in a basic neural network: the input layer, the hidden layer(s), and the output layer. The input layer receives the initial data, which is then processed through the hidden layer(s) before producing the final output in the output layer. This sophisticated structure allows neural networks to learn complex patterns and make predictions.\nEach neuron works by taking in inputs, applying weights and biases to them, performing a mathematical operation, and then passing the result to the next layer. A neuron receives inputs from other neurons or external sources. Each input is associated with a weight, which represents the strength or importance of that input. The neuron computes the weighted sum of its inputs by multiplying each input with its corresponding weight and summing them together. Neurons often have an additional parameter called bias to provide an offset when all inputs are zero. This computation represents the integration of information from multiple sources. The weighted sum is then passed through an activation function. The activation function introduces non-linearity and determines the output of the neuron. It can either be a simple threshold function or a more complex function like sigmoid, ReLU, or tanh. The output of the activation function becomes the output of the neuron. It can be passed as input to other neurons in subsequent layers or used as the final output of the neural network.\n\n\nBackpropagation is a key algorithm used to train neural networks. It works by adjusting the weights of the connections between neurons to minimize the difference between the network’s predicted output and the desired output. Backpropagation uses a technique called gradient descent, which calculates the gradient of the loss function with respect to the network’s weights. This gradient is then used to update the weights iteratively, allowing the network to learn from its mistakes and improve its performance.\n\n\n1.2.2 Traditional machine learning vs deep learning\n\nTraditional machine learning algorithms are generally rule-based systems that rely on explicit feature engineering. These algorithms require human experts to handcraft relevant features from the data, which are then used to train the models. Traditional machine learning techniques excel in scenarios where the data has a limited number of features and the problem at hand can be represented effectively by these engineered features. In contrast, deep learning, which uses multiple layers of interconnected nodes, can automatically learn intricate patterns and representations from raw data, eliminating the need for explicit feature engineering. Deep learning models are particularly effective when dealing with unstructured, high-dimensional data, such as images, audio, and text.\nOne advantage of traditional machine learning approaches is their interpretability. Since the features are explicitly engineered, it is easier to understand how the model arrives at its predictions. This interpretability can be crucial in domains where explanations and justifications are required. On the other hand, deep learning models often function as black boxes, making it challenging to discern the exact reasoning behind their decisions even though they can discover intricate patterns that may be missed by traditional machine learning techniques. It is important to assess the problems before deciding which technique to use.\n\n\n1.2.3 Deep learning limitation\n\nDeep learning is proven to help many organisations from different industries make better decisions based on insights from data such as the cases in detecting frauds in financial transactions or predicting cancer from medical x-rays. Nonetheless, it is not a one-for-all solution as there are use cases when using this technique will not help achieve the best result or is a waste of time. Some of the limitations of deep learning include:\n\nIt usually needs a significant amount of data in order to learn effectively, especially if you want to train a deep learning model.\nThe performance of a model heavily depends on the data it is trained, meaning potential bias might be hard to detect. Overfitting is a common pitfall in machine learning where the model is to specialised in the training data and performs unreliably on test dataset.\nAs some techniques of machine learning, including the artificial neural network, is a black box system, the underlying insights behind the trained model can be hard, or even impossible, to understand. This is also a reason why some techniques are specifically designed for predictions, while others are used for inference."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#model-training-and-evaluation",
    "href": "notebooks/01a-basic-workflow.html#model-training-and-evaluation",
    "title": "1  General Idea",
    "section": "1.3 Model Training and Evaluation",
    "text": "1.3 Model Training and Evaluation\nThe workflow when using deep learning is iterative and dynamic that requires a lot of experimentation and creativity. In this book, there is a certain workflow used to train and evaluate a deep learning model. The general steps are also used in building other traditional machine learning models. The workflow has seven primary components.\n\nAcquiring data: Either collecting by yourself or using existing data, you needs relevant data to solve your business problems. This is typically one of the most important and prone-to-error step in a project. In this book, all data are acquired using publicly available datasets.\nPreprocessing: This step involves cleaning, transforming and organizing your data to make it suitable for your model. More in-depth methods in preprocessing different data types are demonstrated in the Part 3 of this book.\nSplitting and balancing the dataset: Depending on how you approach a problem, in many occasions you might want to separate a subset of available data to function as the test set. Test dataset is a subset used for evaluating the performance of a trained model. Another important subset is the validation set which is used, for instance, to tune hyperparameters or to observere the performance of a model throughout the training process.\nDefining model architecture: For deep learning model, defining model might be arbitrary, but this process can influence the results of the project. In many times, one needs to redefine the architecture iteratively by altering the structure and layers of the neural network in order to achieve the highest performance.\nFitting model to data: This step involves training your model on the training set. You need to specify a loss function that measures how well your model fits the data and select the optimizer that will determines how the model’s parameters are adjusted. In addition, you may want to use particular metrics to evaluate the model performance on the validation set.\nMaking predictions: This is the step when you use your trained model to make predictions on new or unseen data. In this step, developers typically measure how well the model performs on the test set using some metrics such as mean squeared error, accuracy, F1-score, and ROC curve.\nModel optimization: This is intended to improve the model by fine-tuning the hyperparameters, adding regularization techniques such as dropout or batch normalization.\n\nThe processes of training a deep learning model and evaluate its performance on two different problems (regression and classificastion) are discussed in the subsequent chapters."
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#setup",
    "href": "notebooks/01b-basic-regression.html#setup",
    "title": "2  Regression",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nImportant libraries are loaded, namely tensorflow, numpy to manipulate data, pandas to deal with table, and seaborn to create visualisation. As part of the setup, I also clean the TF2 environment with clear_session function and set seed for random number generation using set_random_seed function.\nFor the visualisation, I attempted to use the seaborn.objects interface. The reason for this is that I am familiar with the ggplot2 package in R when conducting data analysis, and I found that there is some similarity in both approach of creating a plot. For those who aren’t familiar with ggplot2 package, it employs the concept of layered grammar of graphics allowing you to describe any plots in a more structured way which result in more convenient and consistent code. You can see the documentations for the seaborn.objects interface here and the ggplot2 package here.\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport seaborn.objects as so\n\n\ntf.keras.backend.clear_session()\ntf.keras.utils.set_random_seed(123)"
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#generate-random-data",
    "href": "notebooks/01b-basic-regression.html#generate-random-data",
    "title": "2  Regression",
    "section": "2.2 Generate Random Data",
    "text": "2.2 Generate Random Data\nFor the sample problem, I generated a 1000 observations of random number with two independent variables \\(x_0\\) and \\(x_1\\). The target for prediction is calculated using simple formula below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\n\nX = np.random.normal(size = (1000, 2))\ny = 0.2 * X[:, 0] + 2.8 * X[:, 1] + np.random.normal(size = (1000,))\n\nFor evaluation of model, I split the data, 80% for training and 20% for testing.\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, \n\n((800, 2), (800,), (200, 2), (200,))"
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#a-very-simple-tf2-model",
    "href": "notebooks/01b-basic-regression.html#a-very-simple-tf2-model",
    "title": "2  Regression",
    "section": "2.3 A Very Simple TF2 Model",
    "text": "2.3 A Very Simple TF2 Model\n\n2.3.1 Defining the model\nDefining a TF2 model can be accomplished easily by calling the Sequential method, followed by adding any types and number of layers. As the problem is very straightforward, tackling this should be relatively easy. For this reason, the model I defined here is very simple with only one Dense layer with one perceptron unit. In addition, we need to define the input_shape so that the model can determine the number of parameters it requires to predict all inputs. The summary method allows you to see the architecture of the model.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 3         \n                                                                 \n=================================================================\nTotal params: 3\nTrainable params: 3\nNon-trainable params: 0\n_________________________________________________________________\n\n\nBefore training the model, you are required to choose the optimizer, loss function, and metrics, and compile those into the model. Here, I decided to use Stochastic Gradient Descent algorithm for optimizing the model (i.e., updating neural network parameters), Mean Squared Error (MSE) for determining how far the prediciton of the current model with actual values, and Mean Absolute Error (MAE) as a metric to evaluate the model.\n\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\n\n\n\n2.3.2 Training the model\nTraining can be done using fit method. The process is done after 100 epochs or cycles of the model updating its parameters based on input data. The verbose parameter that equals 0 means that the training will not print any information.\n\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\nThe fit method returns History object, which provides you the performance of the model during training. The history.history contains all loss and metric scores for all training epochs, and you can extract this information to evalate your model. I performed some manipulation basically to have a certain format of data (the long version, you might want to refer to tidy data by Hadley Wickham).\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\n    \"index\":\"epoch\", \n    \"loss\": \"Mean Squared Error\", \n    \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(\n    data, \n    id_vars = \"epoch\", \n    value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n    var_name = \"metric\",\n    value_name = \"value\"\n)\n\ndata.sort_values(by = \"epoch\").head()\n\n\n\n\n\n  \n    \n      \n      epoch\n      metric\n      value\n    \n  \n  \n    \n      0\n      0\n      Mean Squared Error\n      5.752564\n    \n    \n      100\n      0\n      Mean Absolute Error\n      1.881032\n    \n    \n      1\n      1\n      Mean Squared Error\n      2.836762\n    \n    \n      101\n      1\n      Mean Absolute Error\n      1.315763\n    \n    \n      2\n      2\n      Mean Squared Error\n      1.707698\n    \n  \n\n\n\n\nWe can then visualise how the model perform throughout the training.\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\n\n2.3.3 Evaluating the model\nFinally, we can check the model’s performance on the test dataset. The evaluate method allows the users to see how the model perform when predicting unseen data. The model seems to do good in predicting the actual output based on the MSE and MAE.\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 0.90\nMean Absolute Error: 0.77"
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#dealing-with-non-linearity",
    "href": "notebooks/01b-basic-regression.html#dealing-with-non-linearity",
    "title": "2  Regression",
    "section": "2.4 Dealing with Non-linearity",
    "text": "2.4 Dealing with Non-linearity\nIt is well known that deep learning models are good for high dimensional and complex data. To illustrate the capability of a model in dealing with that type of data, I slightly modified the problem by squaring x_1, giving a non-linear property to the data. The final formula is presented below.\n\\[\n    y = f(x) = 0.2 \\times x_0 + 2.8 \\times x_1^2 + \\epsilon\n\\]\nAll variables \\(x_0\\) and \\(x_1\\) as well as the error term \\(\\epsilon\\) follow a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\).\n\nX = np.random.normal(size = (1000, 2))\ny = 0.2 * X[:, 0] + 2.8 * X[:, 1] ** 2 + np.random.normal(size = (1000,))\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, \n\n((800, 2), (800,), (200, 2), (200,))\n\n\nUsing the same approach as above might not give you the best result as you can see in the graphs below. Both MSE and MAE can be significantly higher compared to the values from the previous problem.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\"index\":\"epoch\", \n                              \"loss\": \"Mean Squared Error\", \n                              \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(data, \n               id_vars = \"epoch\", \n               value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n               var_name = \"metric\",\n               value_name = \"value\")\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 14.94\nMean Absolute Error: 2.75"
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#going-deeper-by-using-more-layers",
    "href": "notebooks/01b-basic-regression.html#going-deeper-by-using-more-layers",
    "title": "2  Regression",
    "section": "2.5 Going Deeper by Using More Layers",
    "text": "2.5 Going Deeper by Using More Layers\nAs the name suggests, Deep Learning techniques leverages several intermediate representation of the data before finally decide what value to assign for any given input. This supports finding complex patterns that are usually inherent in real world data.\nThe previous model is modified simply by adding more Dense layers and increasing the number of the units. The activation function in a model is crucial for capturing non-linearity. The relu activation function is a function that gives either a positive value or zero which is suprisingly effective for balancing the trade-offs between finding non-linear pattern and efficient computation. As each subsequent layer can determine the number of parameters required through inferring the number of units from the previous layer, input_shape is only defined for the first layer.\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (2, )))\nmodel.add(tf.keras.layers.Dense(32, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mae\"])\n\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 32)                96        \n                                                                 \n dense_3 (Dense)             (None, 32)                1056      \n                                                                 \n dense_4 (Dense)             (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 1,185\nTrainable params: 1,185\nNon-trainable params: 0\n_________________________________________________________________\n\n\nAs can be seen in the plots below and the values of MSE and MAE, the ‘deeper’ version of the model could better capture the inherent trend of the dataset leading to more superior model than the previous one.\n\nhistory = model.fit(X_train, y_train, epochs = 100, verbose = 0)\n\ndata = pd.DataFrame(history.history)\ndata = data.reset_index()\ndata = data.rename(columns = {\"index\":\"epoch\", \n                              \"loss\": \"Mean Squared Error\", \n                              \"mae\": \"Mean Absolute Error\"})\ndata = pd.melt(data, \n               id_vars = \"epoch\", \n               value_vars = [\"Mean Squared Error\", \"Mean Absolute Error\"],\n               var_name = \"metric\",\n               value_name = \"value\")\n\n(\n    so.Plot(data, x = \"epoch\", y = \"value\")\n    .facet(\"metric\")\n    .add(so.Line())\n    .share(y = False)\n    .limit(y = (0, None))\n    .layout(size = (8, 3))\n    .label(x = \"\", y = \"\")\n)\n\n\n\n\n\n\n\n\n\nmse, mae = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Mean Squared Error : {mse:.2f}\")\nprint(f\"Mean Absolute Error: {mae:.2f}\")\n\nMean Squared Error : 1.04\nMean Absolute Error: 0.82"
  },
  {
    "objectID": "notebooks/01b-basic-regression.html#conclusion",
    "href": "notebooks/01b-basic-regression.html#conclusion",
    "title": "2  Regression",
    "section": "2.6 Conclusion",
    "text": "2.6 Conclusion\nIn this post, I demonstrate how to leverage a small subset of TensorFlow 2 capabilities to deal with artificial datasets. Even though here only includes problems with structured data with well defined problems and boundaries, Deep Learning model in essence allows anyone to do Machine Learning for highly unstructured data such as images and texts."
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#setup",
    "href": "notebooks/01c-basic-classification.html#setup",
    "title": "3  Classification",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nThe setup includes importing important libraries (tensorflow, numpy, and matplotlib.pyplot), freeing memory from old models/layers (if any) and setting the seed for random number generator.\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntf.keras.backend.clear_session()\ntf.keras.utils.set_random_seed(123)"
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#generate-random-data",
    "href": "notebooks/01c-basic-classification.html#generate-random-data",
    "title": "3  Classification",
    "section": "3.2 Generate Random Data",
    "text": "3.2 Generate Random Data\nFor this use case, I genereated two classes of data based on multivariate normal distribution with specified means and covariance matrices. Both values are determined arbitrarily.\n\n# random data generation\nSAMPLE_SIZE = 1500\n\nclass_1 = np.random.multivariate_normal(\n    mean = [0, 2],\n    cov  = [[1, 0.1], [0.1, 1]],\n    size = SAMPLE_SIZE\n)\n\nclass_2 = np.random.multivariate_normal(\n    mean = [2, 0],\n    cov  = [[1, 0.1], [0.1, 1]],\n    size = SAMPLE_SIZE\n)\n\n# append both classes\nX = np.concatenate([class_1, class_2])\nX = X.astype(\"float32\")\n\ny = np.concatenate([np.zeros((SAMPLE_SIZE, 1)), np.ones((SAMPLE_SIZE, 1))])\ny = y.astype(\"int\")\n\nX.shape, y.shape\n\n((3000, 2), (3000, 1))\n\n\nAs there are only two variables within the data, making sense of it is easier as we only requires a scatter plot to see how data is dispersed along x and y axes. As you can see from the figure below, there is an area where points from class 1 and class 2 overlap.\n\nplt.scatter(X[:, 0], X[:, 1], c = y[:, 0], alpha = .2)\nplt.show()"
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#slice-the-data",
    "href": "notebooks/01c-basic-classification.html#slice-the-data",
    "title": "3  Classification",
    "section": "3.3 Slice the Data",
    "text": "3.3 Slice the Data\nTo help slicing two python variables with the same length (X and y), I created a vector of data indices where the order is shuffled. This then server as a reference to determine which points belong to which datasets (training, validation, or testing).\nI split the data into train and test datasets (80% and 20%), before splitting the train dataset further for hyperparameter tuning into partial train and validation (80% and 20%).\n\n# define randomized indices for splitting\nindices = np.arange(SAMPLE_SIZE * 2)\nnp.random.shuffle(indices)\n\n# split data into `train` and `test datasets`\nsplit_locaction = round(SAMPLE_SIZE * .8)\n\nX_train = X[indices[:split_locaction]]\ny_train = y[indices[:split_locaction]]\n\nX_test = X[indices[split_locaction:]]\ny_test = y[indices[split_locaction:]]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n((1200, 2), (1200, 1), (1800, 2), (1800, 1))\n\n\n\n# split train data into `partial` and `validation` for hyperparameter tuning\nsplit_locaction = round(len(X_train) * .8)\n\npartial_X_train = X_train[:split_locaction]\npartial_y_train = y_train[:split_locaction]\n\nX_val = X_train[split_locaction:]\ny_val = y_train[split_locaction:]\n\npartial_X_train.shape, partial_y_train.shape, X_val.shape, y_val.shape\n\n((960, 2), (960, 1), (240, 2), (240, 1))"
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#hyperparameter-optimization",
    "href": "notebooks/01c-basic-classification.html#hyperparameter-optimization",
    "title": "3  Classification",
    "section": "3.4 Hyperparameter Optimization",
    "text": "3.4 Hyperparameter Optimization\nHyperparameter optimization or tuning can be applied to any parameters controlling the behaviours of the machine learning algorithm which are not learned during training. In doing so, we need to separate the test data and leverage two subsets of training data instead. Otherwise, there might be any leak of information from the ‘unseen data’ which might alter the result of the trained algorithm giving it the capability to perform better on the test dataset. This opposes the idea of ML model that should be able to do well given unknown input, which, in this case is represented as test dataset.\nThe hyperparameter to be tuned is the simple one, in this case number of epochs.The process includes training a network with simplifed architecture, then analyses the performance of the network throughout the training. The optimal number of epochs is decided based on how accuracy and loss values moves throughout time.\nThe actual workflow for creating the model, compiling its optimizer, loss function, and metrics, and fitting it to the data is similar to what you can see from the previous demo. The difference here is that I did not use model.add method to put a layer in the model. Instead, I gave a list of several Dense layers as an argument when instantiating a Sequential model. In addition, the number of units in each layer is a reduced one (we will increase it when training with full train data). I also set the learning rate for the SGD optimizer into 0.005.\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(8, input_shape = (2,), activation = \"relu\"),\n    tf.keras.layers.Dense(8, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.005), \n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nhistory = model.fit(partial_X_train, \n                    partial_y_train, \n                    validation_data=(X_val, y_val), \n                    epochs = 1000, \n                    verbose = 0)\n\nThe model is fitted using the partial_X_train and partial_y_train with a set of validation data. By using validation data, we might observe how the performance of the model throughout training.\nBelow, we can see the values of training and validation accuracy and loss given a certain training epoch. Because of the values for the validation seems to resemble training values, it can be inferred that the model does not overfit. Overfitting may cause the training accuracy to be significantly higher than validation accuracy and training loss to be significantly lower than validation loss.\n\n# Plot the training results\naccuracy     = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nepochs       = range(len(accuracy))\n\nplt.plot(epochs, accuracy, 'r', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.ylim(ymin=0)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the training results\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.ylim(ymin=0)\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#fitting-with-full-training-data",
    "href": "notebooks/01c-basic-classification.html#fitting-with-full-training-data",
    "title": "3  Classification",
    "section": "3.5 Fitting with Full Training Data",
    "text": "3.5 Fitting with Full Training Data\nAfter observing how the simplified model performs, we were able to decide at which epoch we want to stop training our model. In this case, we selected 175 as the subsequent epochs does not give improvement to the model (the loss seemed to stop decreasing). We then could fit our model with full training data and increase the number of units for each Dense layer.\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, input_shape = (2,), activation = \"relu\"),\n    tf.keras.layers.Dense(64, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.005), \n              loss = \"binary_crossentropy\",\n              metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 175, verbose = 0)\n\nNext, we see how the model classifies each data point from the graph below.\n\ny_pred = model.predict(X_test, verbose = 0)\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred[:, 0] > .5, alpha = .3)\nplt.show()\n\n\n\n\n\n\n\n\nWe could also evaluate the performance on the test dataset. The model can reach more than 80% accuracy.\n\nloss, accuracy = model.evaluate(X_test, y_test, verbose = 0)\n\nprint(f\"Loss    : {loss:.3f}\")\nprint(f\"Accuracy: {accuracy:.3f}\")\n\nLoss    : 0.176\nAccuracy: 0.926"
  },
  {
    "objectID": "notebooks/01c-basic-classification.html#conclusion",
    "href": "notebooks/01c-basic-classification.html#conclusion",
    "title": "3  Classification",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nIn this post, we continue our demonstration of TensorFlow 2 with classification problems. The model successfully achieve a decent accuracy score for this simple case. Additionally, we have touched the concept of hyperparameter tuning which is essential for doing machine learning."
  },
  {
    "objectID": "notebooks/02a-mnist.html",
    "href": "notebooks/02a-mnist.html",
    "title": "4  MNIST",
    "section": "",
    "text": "# import libraries\nimport numpy as np\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# transform image data\ntrain_images = train_images.reshape((60000, 28 * 28)) / 255\ntest_images = test_images.reshape((10000, 28 * 28)) / 255\n\ntrain_images.shape, train_labels.shape, test_images.shape, test_labels.shape\n\n\ndef explore(train_images, \n            train_labels, \n            test_images, \n            test_labels,\n            label_count,\n            neuron_count,\n            learning_rate,\n            momentum):\n\n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Dense(neuron_count, activation = \"relu\", input_shape = (28 * 28,)))\n    model.add(layers.Dense(label_count, activation = \"softmax\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    model.compile(optimizer = optimizer,\n                  loss = \"categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n\n    # train ann model\n    history = model.fit(train_images, train_labels, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n\n    return history, test_loss, test_acc\n\n\n# set hyperparameters\nlearning_rates = np.logspace(-1, -4, 5)\nmomentums = np.logspace(-1, -4, 5)\nneuron_counts = 2 ** np.arange(7, 12)\n\nhyparameters_list = []\nfor learning_rate in learning_rates:\n    for momentum in momentums:\n        for neuron_count in neuron_counts:\n            hyparameters_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"neuron_count\": neuron_count\n            })\n\n\noutput = []\nfor hyparameters in hyparameters_list:\n    history, test_loss, test_acc = explore(\n        train_images,\n        train_labels,\n        test_images,\n        test_labels,\n        label_count = 10,\n        learning_rate = hyparameters[\"learning_rate\"],\n        momentum = hyparameters[\"momentum\"],\n        neuron_count = hyparameters[\"neuron_count\"]\n    )\n    \n    output.append({\n        \"history\": history,\n        \"test_loss\": test_loss, \n        \"test_acc\": test_acc,\n        \"hyperparameters\": hyparameters\n    })\n    backend.clear_session()\n\n    print(f\"A model is trained with hyperparameters of {hyparameters}\")"
  },
  {
    "objectID": "notebooks/02b-imdb.html",
    "href": "notebooks/02b-imdb.html",
    "title": "5  IMDB",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\n# preprocess\nX_train = np.zeros(shape = (len(train_data), num_words), dtype = float)\nX_test = np.zeros(shape = (len(test_data), num_words), dtype = float)\n\nfor i, seq in enumerate(train_data):\n    X_train[i, seq] = 1.\n\nfor i, seq in enumerate(test_data):\n    X_test[i, seq] = 1.\n\ny_train = train_labels.astype(float)\ny_test = test_labels.astype(float)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\n\npartial_X_train = X_train[:12500]\npartial_y_train = y_train[:12500]\nX_val = X_train[12500:]\ny_val = y_train[12500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            n_units,\n            n_layers,\n            activation,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    for i in range(n_layers):\n        model.add(layers.Dense(n_units, activation = activation))\n    model.add(layers.Dense(1, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n\n    # train ann model\n    model.build(input_shape = (10000,))\n    model.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nn_unit_list         = [32, 64]\nn_hidden_layer_list = [1, 3]\nactivation_list     = [\"relu\", \"tanh\"]\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for n_units in n_unit_list:\n            for n_layers in n_hidden_layer_list:\n                for activation in activation_list:\n                    param_list.append({\n                        \"learning_rate\": learning_rate,\n                        \"momentum\": momentum,\n                        \"n_units\": n_units,\n                        \"n_layers\": n_layers,\n                        \"activation\": activation\n                    })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        n_units = params[\"n_units\"],\n        n_hidden_layer = params[\"n_hidden_layer\"],\n        activation = params[\"activation\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02c-reuters.html",
    "href": "notebooks/02c-reuters.html",
    "title": "6  Reuters",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import reuters\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels,), (test_data, test_labels) = reuters.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\nseq_len = 300 # the avg is 145.54\n\nX_train = [seq[:seq_len] for seq in train_data]\nX_train = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_train]\nX_train = np.array(X_train).astype(int)\n\ny_train = to_categorical(train_labels)\n\nX_test = [seq[:seq_len] for seq in test_data]\nX_test = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_test]\nX_test = np.array(X_test).astype(int)\n\ny_test = to_categorical(test_labels)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\npartial_X_train = X_train[:4500]\npartial_y_train = y_train[:4500]\nX_val = X_train[4500:]\ny_val = y_train[4500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            embedding_dim,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Embedding(num_words, embedding_dim, input_length = seq_len))\n    model.add(layers.Dense(64, activation = \"relu\"))\n    model.add(layers.Dense(46, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    # train ann model\n    model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nembedding_dim_list  = 2 ** np.arange(3, 7)\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for embedding_dim in embedding_dim_list:\n            param_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"embedding_dim\": embedding_dim\n            })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        embedding_dim = params[\"embedding_dim\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02d-boston.html",
    "href": "notebooks/02d-boston.html",
    "title": "7  Boston Housing",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.base import clone\nfrom tensorflow.keras.datasets import boston_housing\nfrom tensorflow.keras import models, layers, backend\n\n\n# load dataset\n(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\n# rescale and shift data based on training set\ntransform_mean = np.mean(X_train)\ntransform_std  = np.std(X_train, ddof = 1)\n\nX_train -= transform_mean\nX_train /= transform_std\n\nX_test -= transform_mean\nX_test /= transform_std\n\n\nmodel_nn = models.Sequential()\nmodel_nn.add(layers.Dense(128, activation = \"relu\", input_shape = (13,)))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(1))\nmodel_nn.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\n\ninitial_weight_nn = model_nn.get_weights()\n\n\nmodel_nn_reg = models.Sequential()\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", input_shape = (13,)))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(1))\n\nmodel_nn_reg.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\ninitial_weight_nn_reg = model_nn_reg.get_weights()\n\n\nlm_base = LinearRegression()\n\n\nindices = np.arange(len(X_train))\nnp.random.seed(123)\nnp.random.shuffle(indices)\n\nk_fold = 5\nsample_size = np.ceil(len(X_train) / k_fold).astype(int)\n\n\nmse_nn, mse_nn_reg, mse_lm = [], [], []\nmae_nn, mae_nn_reg, mae_lm = [], [], []\n\nfor i in range(k_fold):\n    # configure model with exact parameters\n    model_lm = clone(lm_base)\n    model_nn.set_weights(initial_weight_nn)\n    model_nn_reg.set_weights(initial_weight_nn_reg)\n\n    # split into partial_train and validation\n    id_start, id_end = i * sample_size, (i+1) * sample_size\n\n    mask_train = np.concatenate((indices[:id_start], indices[id_end:]))\n    mask_val   = indices[id_start:id_end]\n\n    X_val = X_train[mask_val]\n    y_val = y_train[mask_val]\n    partial_X_train = X_train[mask_train]\n    partial_y_train = y_train[mask_train]\n\n    # fit and predict\n    model_lm.fit(partial_X_train, partial_y_train)\n    model_nn.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n    model_nn_reg.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n\n    y_pred_lm = model_lm.predict(X_val)\n    y_pred_nn = model_nn.predict(X_val, verbose = 0)\n    y_pred_nn_reg = model_nn_reg.predict(X_val, verbose = 0)\n\n    # save results\n    mse_nn.append(mean_squared_error(y_val, y_pred_nn))\n    mse_nn_reg.append(mean_squared_error(y_val, y_pred_nn_reg))\n    mse_lm.append(mean_squared_error(y_val, y_pred_lm))\n\n    mae_nn.append(mean_absolute_error(y_val, y_pred_nn))\n    mae_nn_reg.append(mean_absolute_error(y_val, y_pred_nn_reg))\n    mae_lm.append(mean_absolute_error(y_val, y_pred_lm))\n\n\nprint(f\"Avg MSE of Neral Network : {np.mean(mse_nn):.2f}\")\nprint(f\"Avg MSE of NN Regulaized : {np.mean(mse_nn_reg):.2f}\")\nprint(f\"Avg MSE of Linear Model  : {np.mean(mse_lm):.2f}\")\n\n\nprint(f\"Avg MAE of Neral Network : {np.mean(mae_nn):.2f}\")\nprint(f\"Avg MAE of NN Regulaized : {np.mean(mae_nn_reg):.2f}\")\nprint(f\"Avg MAE of Linear Model  : {np.mean(mae_lm):.2f}\")"
  }
]