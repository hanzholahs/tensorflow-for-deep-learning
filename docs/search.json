[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TensorFlow 2 for Deep Learning",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "notebooks/01a-basic-regression.html",
    "href": "notebooks/01a-basic-regression.html",
    "title": "1  TensorFlow for Regression Problems",
    "section": "",
    "text": "import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\\[\ny = 2  x_0 + 5.2  x_1 + \\epsilon\n\\]\n\nX = np.random.normal(size = (1000, 2))\ny = 2 * X[:, 0] + 5.2 * X[:, 1] + np.random.normal(size = (1000,))\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mse\", \"mae\"])\nmodel.fit(X_train, y_train, epochs = 300, verbose = 0)\nmodel.evaluate(X_test, y_test)\n\n7/7 [==============================] - 0s 2ms/step - loss: 1.1128 - mse: 1.1128 - mae: 0.8465\n\n\n[1.1127723455429077, 1.1127723455429077, 0.8464701771736145]\n\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(64, input_shape = (2, ), activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(64, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mse\", \"mae\"])\nmodel.fit(X_train, y_train, epochs = 300, verbose = 0)\nmodel.evaluate(X_test, y_test)\n\n7/7 [==============================] - 0s 3ms/step - loss: 1.1905 - mse: 1.1905 - mae: 0.8719\n\n\n[1.1904633045196533, 1.1904633045196533, 0.871860146522522]\n\n\n\\[\ny = 2  x_0 + 5.2  (x_1)^2 + \\epsilon\n\\]\n\nX = np.random.normal(size = (1000, 2))\ny = 2 * X[:, 0] + 5.2 * X[:, 1] ** 2 + np.random.normal(size = (1000,))\n\nX_train = X[:800]\ny_train = y[:800]\nX_test = X[800:]\ny_test = y[800:]\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(1, input_shape = (2, )))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mse\", \"mae\"])\nmodel.fit(X_train, y_train, epochs = 300, verbose = 0)\nmodel.evaluate(X_test, y_test)\n\n7/7 [==============================] - 0s 2ms/step - loss: 44.0046 - mse: 44.0046 - mae: 4.7807\n\n\n[44.004600524902344, 44.004600524902344, 4.780708312988281]\n\n\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(64, input_shape = (2, ), activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(64, activation = \"relu\"))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer = \"sgd\", loss = \"mse\", metrics = [\"mse\", \"mae\"])\nmodel.fit(X_train, y_train, epochs = 300, verbose = 0)\nmodel.evaluate(X_test, y_test)\n\n7/7 [==============================] - 0s 4ms/step - loss: 1.1275 - mse: 1.1275 - mae: 0.8209\n\n\n[1.1274707317352295, 1.1274707317352295, 0.8208972215652466]"
  },
  {
    "objectID": "notebooks/01b-basic-classification.html",
    "href": "notebooks/01b-basic-classification.html",
    "title": "2  TensorFlow for Classification Problems",
    "section": "",
    "text": "import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# random data generation\nSAMPLE_NUMBER = 1000\n\nnegatives = np.random.multivariate_normal(\n    mean = [0, 2],\n    cov  = [[1, 0.5], [0.5, 1]],\n    size = SAMPLE_NUMBER\n)\n\npositives = np.random.multivariate_normal(\n    mean = [2.5, 0],\n    cov  = [[1, 0.5], [0.5, 1]],\n    size = SAMPLE_NUMBER\n)\n\n\nXs = np.vstack((negatives, positives)).astype(np.float32)\nys = np.vstack((np.zeros((SAMPLE_NUMBER, 1), dtype = np.float32),\n                np.ones(( SAMPLE_NUMBER, 1), dtype = np.float32)))\n\nXs.shape, ys.shape\n\n((2000, 2), (2000, 1))\n\n\n\nplt.scatter(Xs[:, 0], Xs[:, 1], c = ys[:, 0], alpha = .5)\n\n<matplotlib.collections.PathCollection at 0x24962914340>\n\n\n\n\n\n\n# build the linear classifier\ninput_dim = 2\noutput_dim = 1\nlearning_rate = 0.1\n\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(64, input_shape = (2,), activation = \"relu\"),\n    tf.keras.layers.Dense(64, activation = \"relu\"),\n    tf.keras.layers.Dense(64, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"acc\"])\n\n\nmodel.fit(Xs, ys, epochs = 500, verbose = 0)\n\n<keras.callbacks.History at 0x24963c8fa30>\n\n\n\ny_pred = model.predict(Xs)\n\n63/63 [==============================] - 0s 1ms/step\n\n\n\nmodel.evaluate(Xs, ys)\n\n63/63 [==============================] - 0s 2ms/step - loss: 0.0146 - acc: 0.9945\n\n\n[0.01458920631557703, 0.9944999814033508]\n\n\n\nplt.scatter(Xs[:, 0], Xs[:, 1], c = y_pred[:, 0] > .5, alpha = .5)\n\n<matplotlib.collections.PathCollection at 0x248e66194c0>"
  },
  {
    "objectID": "notebooks/02a-mnist.html",
    "href": "notebooks/02a-mnist.html",
    "title": "3  MNIST",
    "section": "",
    "text": "# import libraries\nimport numpy as np\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# transform image data\ntrain_images = train_images.reshape((60000, 28 * 28)) / 255\ntest_images = test_images.reshape((10000, 28 * 28)) / 255\n\ntrain_images.shape, train_labels.shape, test_images.shape, test_labels.shape\n\n\ndef explore(train_images, \n            train_labels, \n            test_images, \n            test_labels,\n            label_count,\n            neuron_count,\n            learning_rate,\n            momentum):\n\n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Dense(neuron_count, activation = \"relu\", input_shape = (28 * 28,)))\n    model.add(layers.Dense(label_count, activation = \"softmax\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    model.compile(optimizer = optimizer,\n                  loss = \"categorical_crossentropy\",\n                  metrics = [\"accuracy\"])\n\n    # train ann model\n    history = model.fit(train_images, train_labels, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n\n    return history, test_loss, test_acc\n\n\n# set hyperparameters\nlearning_rates = np.logspace(-1, -4, 5)\nmomentums = np.logspace(-1, -4, 5)\nneuron_counts = 2 ** np.arange(7, 12)\n\nhyparameters_list = []\nfor learning_rate in learning_rates:\n    for momentum in momentums:\n        for neuron_count in neuron_counts:\n            hyparameters_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"neuron_count\": neuron_count\n            })\n\n\noutput = []\nfor hyparameters in hyparameters_list:\n    history, test_loss, test_acc = explore(\n        train_images,\n        train_labels,\n        test_images,\n        test_labels,\n        label_count = 10,\n        learning_rate = hyparameters[\"learning_rate\"],\n        momentum = hyparameters[\"momentum\"],\n        neuron_count = hyparameters[\"neuron_count\"]\n    )\n    \n    output.append({\n        \"history\": history,\n        \"test_loss\": test_loss, \n        \"test_acc\": test_acc,\n        \"hyperparameters\": hyparameters\n    })\n    backend.clear_session()\n\n    print(f\"A model is trained with hyperparameters of {hyparameters}\")"
  },
  {
    "objectID": "notebooks/02b-imdb.html",
    "href": "notebooks/02b-imdb.html",
    "title": "4  IMDB",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\n# preprocess\nX_train = np.zeros(shape = (len(train_data), num_words), dtype = float)\nX_test = np.zeros(shape = (len(test_data), num_words), dtype = float)\n\nfor i, seq in enumerate(train_data):\n    X_train[i, seq] = 1.\n\nfor i, seq in enumerate(test_data):\n    X_test[i, seq] = 1.\n\ny_train = train_labels.astype(float)\ny_test = test_labels.astype(float)\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\n\npartial_X_train = X_train[:12500]\npartial_y_train = y_train[:12500]\nX_val = X_train[12500:]\ny_val = y_train[12500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            n_units,\n            n_layers,\n            activation,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    for i in range(n_layers):\n        model.add(layers.Dense(n_units, activation = activation))\n    model.add(layers.Dense(1, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n\n    # train ann model\n    model.build(input_shape = (10000,))\n    model.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nn_unit_list         = [32, 64]\nn_hidden_layer_list = [1, 3]\nactivation_list     = [\"relu\", \"tanh\"]\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for n_units in n_unit_list:\n            for n_layers in n_hidden_layer_list:\n                for activation in activation_list:\n                    param_list.append({\n                        \"learning_rate\": learning_rate,\n                        \"momentum\": momentum,\n                        \"n_units\": n_units,\n                        \"n_layers\": n_layers,\n                        \"activation\": activation\n                    })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        n_units = params[\"n_units\"],\n        n_hidden_layer = params[\"n_hidden_layer\"],\n        activation = params[\"activation\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02c-reuters.html",
    "href": "notebooks/02c-reuters.html",
    "title": "5  Reuters",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.datasets import reuters\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models, layers, optimizers, backend\n\n\n# load dataset\nnum_words = 10000\n\n(train_data, train_labels,), (test_data, test_labels) = reuters.load_data(num_words = num_words)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape\n\n\nseq_len = 300 # the avg is 145.54\n\nX_train = [seq[:seq_len] for seq in train_data]\nX_train = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_train]\nX_train = np.array(X_train).astype(int)\n\ny_train = to_categorical(train_labels)\n\nX_test = [seq[:seq_len] for seq in test_data]\nX_test = [np.append([0] * (seq_len - len(seq)), seq) for seq in X_test]\nX_test = np.array(X_test).astype(int)\n\ny_test = to_categorical(test_labels)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\npartial_X_train = X_train[:4500]\npartial_y_train = y_train[:4500]\nX_val = X_train[4500:]\ny_val = y_train[4500:]\n\n\ndef explore(X_train, \n            y_train,\n            X_val,\n            y_val,\n            embedding_dim,\n            learning_rate,\n            momentum):\n    \n    # define ann architecture\n    model = models.Sequential()\n    model.add(layers.Embedding(num_words, embedding_dim, input_length = seq_len))\n    model.add(layers.Dense(64, activation = \"relu\"))\n    model.add(layers.Dense(46, activation = \"sigmoid\"))\n\n    # define optimizer, loss function, and metrics\n    optimizer = optimizers.RMSprop(learning_rate = learning_rate, momentum = momentum)\n\n    # train ann model\n    model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n    # evaluate ann model\n    val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 0)\n\n    return val_loss, val_acc\n\n\n# set hyperparameters\nlearning_rate_list  = np.logspace(-2, -4, 5)\nmomentum_list       = np.linspace(0.1, 0.9, 5)\nembedding_dim_list  = 2 ** np.arange(3, 7)\n\nparam_list = []\nfor learning_rate in learning_rate_list:\n    for momentum in momentum_list:\n        for embedding_dim in embedding_dim_list:\n            param_list.append({\n                \"learning_rate\": learning_rate,\n                \"momentum\": momentum,\n                \"embedding_dim\": embedding_dim\n            })\n\n\nresults = []\nfor params in param_list:\n    val_loss, val_acc = explore(\n        partial_X_train, \n        partial_y_train,\n        X_val,\n        y_val,\n        embedding_dim = params[\"embedding_dim\"],\n        learning_rate = params[\"learning_rate\"],\n        momentum = params[\"momentum\"],\n    )\n    \n    results.append({\"val_loss\": val_loss,\n                    \"val_acc\": val_acc,\n                    \"params\": params})\n\n    backend.clear_session()\n\n\n# get optimal parameters\nval_accuracies = [result[\"val_acc\"] for result in results]\nopt_params     = results[np.argmax(val_accuracies)][\"params\"]\n\nopt_params\n\n\n# define ann architecture\nmodel = models.Sequential()\nfor i in range(opt_params[\"n_layers\"]):\n    model.add(layers.Dense(opt_params[\"n_units\"], activation = opt_params[\"activation\"]))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\n# define optimizer, loss function, and metrics\noptimizer = optimizers.RMSprop(learning_rate = opt_params[\"learning_rate\"], \n                               momentum = opt_params[\"momentum\"])\n\n# train ann model\nmodel.build(input_shape = (10000,))\nmodel.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs = 20, batch_size = 64, verbose = 0)\n\n\nloss = history['loss']\n\nepochs = range(1, len(loss) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, loss, solid_blue_line, label = 'Training loss')\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\n\naccuracy = history['accuracy']\n\nepochs = range(1, len(accuracy) + 1)\n\nblue_dots = 'bo'\nsolid_blue_line = 'b'\n\nplt.plot(epochs, accuracy, solid_blue_line, label = 'Training accuracy')\nplt.title('Training accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()\n\n\nmodel.evaluate(X_test, y_test)"
  },
  {
    "objectID": "notebooks/02d-boston.html",
    "href": "notebooks/02d-boston.html",
    "title": "6  Boston Housing",
    "section": "",
    "text": "# import libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.base import clone\nfrom tensorflow.keras.datasets import boston_housing\nfrom tensorflow.keras import models, layers, backend\n\n\n# load dataset\n(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n\n# rescale and shift data based on training set\ntransform_mean = np.mean(X_train)\ntransform_std  = np.std(X_train, ddof = 1)\n\nX_train -= transform_mean\nX_train /= transform_std\n\nX_test -= transform_mean\nX_test /= transform_std\n\n\nmodel_nn = models.Sequential()\nmodel_nn.add(layers.Dense(128, activation = \"relu\", input_shape = (13,)))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(128, activation = \"relu\"))\nmodel_nn.add(layers.Dense(1))\nmodel_nn.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\n\ninitial_weight_nn = model_nn.get_weights()\n\n\nmodel_nn_reg = models.Sequential()\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", input_shape = (13,)))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(64, activation = \"relu\", kernel_regularizer='l1_l2'))\nmodel_nn_reg.add(layers.Dropout(0.3))\nmodel_nn_reg.add(layers.Dense(1))\n\nmodel_nn_reg.compile(optimizer = \"adam\", loss = \"mse\", metrics = [\"mae\", \"mse\"])\ninitial_weight_nn_reg = model_nn_reg.get_weights()\n\n\nlm_base = LinearRegression()\n\n\nindices = np.arange(len(X_train))\nnp.random.seed(123)\nnp.random.shuffle(indices)\n\nk_fold = 5\nsample_size = np.ceil(len(X_train) / k_fold).astype(int)\n\n\nmse_nn, mse_nn_reg, mse_lm = [], [], []\nmae_nn, mae_nn_reg, mae_lm = [], [], []\n\nfor i in range(k_fold):\n    # configure model with exact parameters\n    model_lm = clone(lm_base)\n    model_nn.set_weights(initial_weight_nn)\n    model_nn_reg.set_weights(initial_weight_nn_reg)\n\n    # split into partial_train and validation\n    id_start, id_end = i * sample_size, (i+1) * sample_size\n\n    mask_train = np.concatenate((indices[:id_start], indices[id_end:]))\n    mask_val   = indices[id_start:id_end]\n\n    X_val = X_train[mask_val]\n    y_val = y_train[mask_val]\n    partial_X_train = X_train[mask_train]\n    partial_y_train = y_train[mask_train]\n\n    # fit and predict\n    model_lm.fit(partial_X_train, partial_y_train)\n    model_nn.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n    model_nn_reg.fit(partial_X_train, partial_y_train, epochs = 500, verbose = 0)\n\n    y_pred_lm = model_lm.predict(X_val)\n    y_pred_nn = model_nn.predict(X_val, verbose = 0)\n    y_pred_nn_reg = model_nn_reg.predict(X_val, verbose = 0)\n\n    # save results\n    mse_nn.append(mean_squared_error(y_val, y_pred_nn))\n    mse_nn_reg.append(mean_squared_error(y_val, y_pred_nn_reg))\n    mse_lm.append(mean_squared_error(y_val, y_pred_lm))\n\n    mae_nn.append(mean_absolute_error(y_val, y_pred_nn))\n    mae_nn_reg.append(mean_absolute_error(y_val, y_pred_nn_reg))\n    mae_lm.append(mean_absolute_error(y_val, y_pred_lm))\n\n\nprint(f\"Avg MSE of Neral Network : {np.mean(mse_nn):.2f}\")\nprint(f\"Avg MSE of NN Regulaized : {np.mean(mse_nn_reg):.2f}\")\nprint(f\"Avg MSE of Linear Model  : {np.mean(mse_lm):.2f}\")\n\n\nprint(f\"Avg MAE of Neral Network : {np.mean(mae_nn):.2f}\")\nprint(f\"Avg MAE of NN Regulaized : {np.mean(mae_nn_reg):.2f}\")\nprint(f\"Avg MAE of Linear Model  : {np.mean(mae_lm):.2f}\")"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]