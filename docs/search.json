[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning with TensorFlow 2",
    "section": "",
    "text": "Welcome\nWelcome to for Deep Learning using TensorFlow 2, a book that will show you how to use TensorFlow 2 to build and train your own models. In this book, you will learn how to use TensorFlow 2 and its high-level API, Keras, to implement various deep learning techniques, such as artificial neural networks, convolutional neural networks, recurrent neural networks, and more. In addition, several approaches to deal with different types of data, such as image, text, and time series data, will be discussed. You will learn the fundamental of TensorFlow 2 for use cases in some of the most popular and challenging domains of deep learning, which are computer vision, natural language processing, and time series data.\nThis book is designed for beginners who want to learn TensorFlow 2 from scratch by practicing the techniques on practical examples and projects. Whether you are new to TensorFlow or have some experience with it, this book will help you master the essential skills and concepts you need to create effective solutions for real-world problems. Nevertheless, you are assumed to have some backgrounds in Python programming. Each chapter provides a brief introduction to the theory behind each technique, followed by a step-by-step guide on how to implement it using TensorFlow 2, Keras, and other relevant libraries.\nBy the end of this book, you will have a solid foundation in TensorFlow 2 and deep learning, and you will be able to use your skills and knowledge to create your own projects or advance your career in this exciting and fast-growing field.\n\nCollaboration\nThis book is currently under development. The source code for this website is available on GitHub. We welcome any contributions or feedback from other developers who are interested in this project. If you find any errors or typos, please feel free to open an issue or a pull request. Thank you for your support and collaboration.\n\n\nAuthors\n\nHanzholah Shobri, a Researcher at Bank Indonesia. He hold a bachelor’s degree in Industrial Engineering from Universitas Gadjah Mada, Yogyakarta. (contacts: email, github, twitter)\nTria Rahmat Mauludin, a Researcher at Bank Indonesia. He hold a bachelor’s degree in Mathematics from Insitut Teknologi Bandung, Bandung. (contacts: email, github)\n\n\n\nLicense\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nThe code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html",
    "href": "notebooks/01a-basic-workflow.html",
    "title": "1  General Workflow for This Book",
    "section": "",
    "text": "Deep learning is a powerful and popular branch of machine learning that uses neural networks to learn from data and perform complex tasks. In this blog post, we will explore the typical workflow of a deep learning project, and discuss some of the relevant libraries, tools and techniques that can help you along the way.\nThe deep learning workflow has seven primary components:\n\nAcquiring data: This is the first and often the most challenging step of a deep learning project. You need to collect enough labeled data for your task, or use existing datasets that are publicly available or stored in databases. You can also use web scraping or APIs to gather data from online sources, or crowd-source labeling for unlabeled data.\nPreprocessing: This step involves cleaning, transforming and organizing your data to make it suitable for your model. You may need to resize, crop, rotate, augment or normalize your images; tokenize, lemmatize, stem or vectorize your text; encode, scale, impute or standardize your numerical data; and so on. You can use libraries like NumPy, Pandas, OpenCV, PIL, NLTK or Scikit-learn for preprocessing tasks.\nSplitting and balancing the dataset: This step involves dividing your data into training, validation and test sets. The training set is used to fit the model to the data; the validation set is used to tune the hyperparameters and select the best model; and the test set is used to evaluate the final performance of the model. You should also ensure that your dataset is balanced, meaning that it has roughly equal proportions of different classes or categories. You can use Scikit-learn’s train_test_split function or TensorFlow’s tf.data API for splitting and balancing your dataset.\nDefining model architecture: This step involves designing the structure and layers of your neural network. You can use existing architectures that have been proven to work well for similar tasks, such as CNNs for image classification, RNNs for natural language processing, GANs for image generation, etc. You can also modify or customize these architectures to suit your specific needs. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for defining your model architecture.\nFitting model to data: This step involves training your model on the training set using an optimization algorithm such as gradient descent. You need to specify the loss function that measures how well your model fits the data, and the metrics that evaluate how well your model performs on the validation set. You also need to choose the learning rate, batch size, number of epochs and other hyperparameters that affect the speed and quality of training. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for fitting your model to data.\nMaking predictions: This step involves using your trained model to make predictions on new or unseen data. You can use your model to classify images, generate text, translate languages, detect objects, etc. depending on your task. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for making predictions.\nEvaluating results: This step involves measuring how well your model performs on the test set using various metrics such as accuracy, precision, recall, F1-score, ROC curve, etc. depending on your task. You can also compare your results with existing benchmarks or state-of-the-art models to see how you fare against them. You can use libraries like Scikit-learn, TensorFlow or PyTorch for evaluating results.\nModel optimization: This step involves improving your model by fine-tuning the hyperparameters, adding regularization techniques such as dropout or batch normalization, pruning unnecessary weights or layers, etc. You can also use techniques such as transfer learning or meta-learning to leverage existing knowledge or learn from multiple tasks. You can use libraries like TensorFlow Hub or PyTorch Lightning for model optimization.\n\nAs you can see, deep learning workflow is an iterative and dynamic process that requires a lot of experimentation and creativity. However, by following these steps and using these libraries and tools, you can make your deep learning project more productive, reproducible and understandable."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Deep learning is a branch of artificial intelligence that uses neural networks to learn from data and perform certain tasks. Many creative and innnovative solutions to human problems have been solved by deep learning techniques. Hence, the skills to use deep learning in any domain is important in the era where computational capabilty and data availability is continally increasing. This book consists of the application of several key techniques in deep learning to deal with complex data structures.\nIf you are familiar with Python and want to learn TensorFlow by solving real-world problems, this book is for you. You will learn how to use TensorFlow to build machine learning models for various domains, such as computer vision, natural language processing, and more. You will also get hands-on experience with TensorFlow through case studies and practical examples that show you how to apply the concepts and techniques in your own projects. Every relevant concept is introduced gradually, so it is recommended to read the book from the first chapter to the end.\n\nThe concepts of Artificial Intelligence, Machine Learning, and Deep Learning\nTo understand what deep learning is, we need to also look at the definition of machine learning and deep learning. In principle, deep learning is a subset of machine learning, and machine learning is a subset of artificial intelligence.\nArtificial intelligence (AI) is a broad term that refers to any system or software that can perform tasks that normally require human intelligence, such as reasoning, decision making, problem solving, natural language processing, and computer vision. Machine learning (ML) is a subset of AI that focuses on creating systems or software that can learn patterns or rules from data and improve their performance without explicit programming or human intervention.\nDeep learning (DL) is a subset of ML that uses artificial neural networks (ANNs) to model complex and nonlinear relationships between inputs and outputs. In contrast to traditional machine learning that requires manual feature engineering to extract relevant information, deep learning uses multiple intermediate representations of the data to extract features automatically from raw data.\nAn illustration by Data Iku on the relationship between AI, ML, and DL is presented below.\n\n\n\nDL is a subset of ML, and ML is a subset of AI\n\n\n\n\nAn Introduction to TensorFlow 2.0 and Keras Libraries\nTensorflow 2 and Keras are two powerful frameworks for building and deploying machine learning solutions, especially for deep learning. Tensorflow 2 is the premier open-source platform for developing, training, and deploying deep learning models at scale. Keras is the high-level API of Tensorflow 2, which provides an easy and intuitive way to design, fit, evaluate, and use deep learning models with just a few lines of code.\nTensorflow 2 and Keras have many advantages over other frameworks, such as:\n\nThey support multiple backends, devices, and languages, allowing you to run your code on CPUs, GPUs, TPUs, and even mobile devices.\nThey offer a rich set of tools and libraries for data processing, visualization, debugging, optimization, and deployment.\nThey enable you to use both the sequential and functional APIs to create simple or complex models with different architectures and layers.\nThey allow you to leverage the power of custom layers, custom training loops, custom callbacks, and custom metrics to customize your model according to your needs.\nThey integrate seamlessly with other popular libraries and frameworks, such as NumPy, SciPy, Pandas, Scikit-learn, PyTorch, and more.\n\nTensorflow 2 and Keras are not only frameworks but also communities of developers, researchers, and practitioners who share their knowledge and experience through blogs, tutorials, books, courses, forums, and events. By using Tensorflow 2 and Keras, you can join this vibrant community and learn from the best in the field.\n\n\nHow Will You Learn Deep Learning by Using This Book?\nThis book exclusively uses TensorFlow 2, as well as its high-level API or Keras, to build and train a deep learning model. Other libraries are also used for preparing the data or making visualisation. To practice with the code in this book, readers could access the source notebook or copy and paste each code manually. There are three parts of the book.\n\nPart 1: Basic Usage of TensorFlow 2. This part contains the basic workflow to build a model that is used in this book and how to implement TensorFlow 2 according to the workflow for regression and classification problems using dummy data.\nPart 2: Use Cases of TensorFlow 2. This part demonstrates several applications of some important techniques in machine learning and TensorFlow 2 to four use cases using datasets provided by TensorFlow 2 library.\nPart 3: Preparing Data for TensorFlow 2. This part goes beyond built-in dataset and shows how to handle messy, real-world data in different formats (images, texts, and time series data) into inputs for TensorFlow 2 deep learning models."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#objectives-of-machine-learning",
    "href": "notebooks/01a-basic-workflow.html#objectives-of-machine-learning",
    "title": "1  General Workflow for This Book",
    "section": "1.1 Objectives of Machine Learning",
    "text": "1.1 Objectives of Machine Learning\nDeep learning is a powerful and popular branch of machine learning that uses neural networks to learn from data and perform complex tasks. In this blog post, we will explore the typical workflow of a deep learning project, and discuss some of the relevant libraries, tools and techniques that can help you along the way.\nThe deep learning workflow has seven primary components:\n\nAcquiring data: This is the first and often the most challenging step of a deep learning project. You need to collect enough labeled data for your task, or use existing datasets that are publicly available or stored in databases. You can also use web scraping or APIs to gather data from online sources, or crowd-source labeling for unlabeled data.\nPreprocessing: This step involves cleaning, transforming and organizing your data to make it suitable for your model. You may need to resize, crop, rotate, augment or normalize your images; tokenize, lemmatize, stem or vectorize your text; encode, scale, impute or standardize your numerical data; and so on. You can use libraries like NumPy, Pandas, OpenCV, PIL, NLTK or Scikit-learn for preprocessing tasks.\nSplitting and balancing the dataset: This step involves dividing your data into training, validation and test sets. The training set is used to fit the model to the data; the validation set is used to tune the hyperparameters and select the best model; and the test set is used to evaluate the final performance of the model. You should also ensure that your dataset is balanced, meaning that it has roughly equal proportions of different classes or categories. You can use Scikit-learn’s train_test_split function or TensorFlow’s tf.data API for splitting and balancing your dataset.\nDefining model architecture: This step involves designing the structure and layers of your neural network. You can use existing architectures that have been proven to work well for similar tasks, such as CNNs for image classification, RNNs for natural language processing, GANs for image generation, etc. You can also modify or customize these architectures to suit your specific needs. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for defining your model architecture.\nFitting model to data: This step involves training your model on the training set using an optimization algorithm such as gradient descent. You need to specify the loss function that measures how well your model fits the data, and the metrics that evaluate how well your model performs on the validation set. You also need to choose the learning rate, batch size, number of epochs and other hyperparameters that affect the speed and quality of training. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for fitting your model to data.\nMaking predictions: This step involves using your trained model to make predictions on new or unseen data. You can use your model to classify images, generate text, translate languages, detect objects, etc. depending on your task. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for making predictions.\nEvaluating results: This step involves measuring how well your model performs on the test set using various metrics such as accuracy, precision, recall, F1-score, ROC curve, etc. depending on your task. You can also compare your results with existing benchmarks or state-of-the-art models to see how you fare against them. You can use libraries like Scikit-learn, TensorFlow or PyTorch for evaluating results.\nModel optimization: This step involves improving your model by fine-tuning the hyperparameters, adding regularization techniques such as dropout or batch normalization, pruning unnecessary weights or layers, etc. You can also use techniques such as transfer learning or meta-learning to leverage existing knowledge or learn from multiple tasks. You can use libraries like TensorFlow Hub or PyTorch Lightning for model optimization.\n\nAs you can see, deep learning workflow is an iterative and dynamic process that requires a lot of experimentation and creativity. However, by following these steps and using these libraries and tools, you can make your deep learning project more productive, reproducible and understandable."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#types-of-machine-learning-problems",
    "href": "notebooks/01a-basic-workflow.html#types-of-machine-learning-problems",
    "title": "1  General Idea of Machine Learning",
    "section": "1.1 Types of Machine Learning Problems",
    "text": "1.1 Types of Machine Learning Problems\nFor developers, data scientists, or researchers, machine learning techniques offer them with the capability to handle complex problems by training a mathematical model that can extract some rules or patterns within a sample and use those to generate some predictions. This usually goes beyond the traditional statistical methods that was invented to deal with a smaller amount of data. The main goal here is to achieve the optimal result when the model is deployed in real-world situation, i.e., the model has to be able to predict samples that are not used for the training process. It is important to understand different approaches in machine learning problems.\nMachine Learning problems can generally be classified into three, namely supervised learning, unsupervised learning, and reinforcement learning.\n\nIn a supervised learning, the model usually is given a dataset containing sets of feature data and its label. The system is trained in order to learn a mapping rules of the inputs to the respective outputs so that it can infer the label of a new, previously-unseen observations.\nIn unsupervised learning, the model processes raw data that is neither labeled nor tagged by human. The purpose of this type of problem is to find hidden patterns or structure in the data.\nIn reinforcement learning, the model is trained to be the brain of an independent agent which interact with certain environment by performing some actions, observing the impacts of its actions in the environment, and receiving reward or penalties based on its actions.\n\nOne thing to note is that sometimes the lines between these types of problem can be blurry. For example, one can deal with data that is half unlabeled and half labeled in a semi-supervised learning which is a mixture of both supervised and unsupervised learning."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#machine-learning-workflow",
    "href": "notebooks/01a-basic-workflow.html#machine-learning-workflow",
    "title": "1  General Idea of Machine Learning",
    "section": "1.2 Machine Learning Workflow",
    "text": "1.2 Machine Learning Workflow\nDeep learning is a powerful and popular branch of machine learning that uses neural networks to learn from data and perform complex tasks. In this blog post, we will explore the typical workflow of a deep learning project, and discuss some of the relevant libraries, tools and techniques that can help you along the way.\nThe deep learning workflow has seven primary components:\n\nAcquiring data: This is the first and often the most challenging step of a deep learning project. You need to collect enough labeled data for your task, or use existing datasets that are publicly available or stored in databases. You can also use web scraping or APIs to gather data from online sources, or crowd-source labeling for unlabeled data.\nPreprocessing: This step involves cleaning, transforming and organizing your data to make it suitable for your model. You may need to resize, crop, rotate, augment or normalize your images; tokenize, lemmatize, stem or vectorize your text; encode, scale, impute or standardize your numerical data; and so on. You can use libraries like NumPy, Pandas, OpenCV, PIL, NLTK or Scikit-learn for preprocessing tasks.\nSplitting and balancing the dataset: This step involves dividing your data into training, validation and test sets. The training set is used to fit the model to the data; the validation set is used to tune the hyperparameters and select the best model; and the test set is used to evaluate the final performance of the model. You should also ensure that your dataset is balanced, meaning that it has roughly equal proportions of different classes or categories. You can use Scikit-learn’s train_test_split function or TensorFlow’s tf.data API for splitting and balancing your dataset.\nDefining model architecture: This step involves designing the structure and layers of your neural network. You can use existing architectures that have been proven to work well for similar tasks, such as CNNs for image classification, RNNs for natural language processing, GANs for image generation, etc. You can also modify or customize these architectures to suit your specific needs. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for defining your model architecture.\nFitting model to data: This step involves training your model on the training set using an optimization algorithm such as gradient descent. You need to specify the loss function that measures how well your model fits the data, and the metrics that evaluate how well your model performs on the validation set. You also need to choose the learning rate, batch size, number of epochs and other hyperparameters that affect the speed and quality of training. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for fitting your model to data.\nMaking predictions: This step involves using your trained model to make predictions on new or unseen data. You can use your model to classify images, generate text, translate languages, detect objects, etc. depending on your task. You can use libraries like TensorFlow, Keras, PyTorch or MXNet for making predictions.\nEvaluating results: This step involves measuring how well your model performs on the test set using various metrics such as accuracy, precision, recall, F1-score, ROC curve, etc. depending on your task. You can also compare your results with existing benchmarks or state-of-the-art models to see how you fare against them. You can use libraries like Scikit-learn, TensorFlow or PyTorch for evaluating results.\nModel optimization: This step involves improving your model by fine-tuning the hyperparameters, adding regularization techniques such as dropout or batch normalization, pruning unnecessary weights or layers, etc. You can also use techniques such as transfer learning or meta-learning to leverage existing knowledge or learn from multiple tasks. You can use libraries like TensorFlow Hub or PyTorch Lightning for model optimization.\n\nAs you can see, deep learning workflow is an iterative and dynamic process that requires a lot of experimentation and creativity. However, by following these steps and using these libraries and tools, you can make your deep learning project more productive, reproducible and understandable."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#machine-learning-problems",
    "href": "notebooks/01a-basic-workflow.html#machine-learning-problems",
    "title": "1  General Idea of Machine Learning",
    "section": "1.1 Machine Learning Problems",
    "text": "1.1 Machine Learning Problems\nFor developers, data scientists, or researchers, machine learning techniques offer them with the capability to handle complex problems by training a mathematical model that can extract some rules or patterns within a sample and use those to generate some predictions. This usually goes beyond the traditional statistical methods that was invented to deal with a smaller amount of data. The main goal here is to achieve the optimal result when the model is deployed in real-world situation, i.e., the model has to be able to predict samples that are not used for the training process. It is important to understand different approaches in machine learning problems.\nMachine Learning problems can generally be classified into three, namely supervised learning, unsupervised learning, and reinforcement learning.\n\nIn a supervised learning, the model usually is given a dataset containing sets of feature data and its label. The system is trained in order to learn a mapping rules of the inputs to the respective outputs so that it can infer the label of a new, previously-unseen observations.\nIn unsupervised learning, the model processes raw data that is neither labeled nor tagged by human. The purpose of this type of problem is to find hidden patterns or structure in the data.\nIn reinforcement learning, the model is trained to be the brain of an independent agent which interact with certain environment by performing some actions, observing the impacts of its actions in the environment, and receiving reward or penalties based on its actions.\n\nOne thing to note is that sometimes the lines between these types of problem can be blurry. For example, one can deal with data that is half unlabeled and half labeled in a semi-supervised learning which is a mixture of both supervised and unsupervised learning.\nThis book discusses several use cases of deep learning in different applications, including in natural language processing, computer vision, and time series analysis. However, each use case is supervised learning where the model is trained using labeled input data and used to predict new dataset. Specifically, all techniques learned in this book is used for either regression or classification problems. Regression is a type of problem where the model needs to predict continuous, numerical data whereas, for classification problem, the model has to infer categorical label of the data."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#the-workflow",
    "href": "notebooks/01a-basic-workflow.html#the-workflow",
    "title": "1  General Idea of Machine Learning",
    "section": "1.2 The Workflow",
    "text": "1.2 The Workflow\nThe workflow when using deep learning is iterative and dynamic that requires a lot of experimentation and creativity. In this book, there is a certain workflow used to train and evaluate a deep learning model. The general steps are also used in building other traditional machine learning models. The workflow has seven primary components.\n\nAcquiring data: Either collecting by yourself or using existing data, you needs relevant data to solve your business problems. This is typically one of the most important and prone-to-error step in a project. In this book, all data are acquired using publicly available datasets.\nPreprocessing: This step involves cleaning, transforming and organizing your data to make it suitable for your model. More in-depth methods in preprocessing different data types are demonstrated in the Part 3 of this book.\nSplitting and balancing the dataset: Depending on how you approach a problem, in many occasions you might want to separate a subset of available data to function as the test set. Test dataset is a subset used for evaluating the performance of a trained model. Another important subset is the validation set which is used, for instance, to tune hyperparameters or to observere the performance of a model throughout the training process.\nDefining model architecture: For deep learning model, defining model might be arbitrary, but this process can influence the results of the project. In many times, one needs to redefine the architecture iteratively by altering the structure and layers of the neural network in order to achieve the highest performance.\nFitting model to data: This step involves training your model on the training set. You need to specify a loss function that measures how well your model fits the data and select the optimizer that will determines how the model’s parameters are adjusted. In addition, you may want to use particular metrics to evaluate the model performance on the validation set.\nMaking predictions: This is the step when you use your trained model to make predictions on new or unseen data. In this step, developers typically measure how well the model performs on the test set using some metrics such as mean squeared error, accuracy, F1-score, and ROC curve.\nModel optimization: This is intended to improve the model by fine-tuning the hyperparameters, adding regularization techniques such as dropout or batch normalization."
  },
  {
    "objectID": "notebooks/01a-basic-workflow.html#the-limitations-of-the-techniques",
    "href": "notebooks/01a-basic-workflow.html#the-limitations-of-the-techniques",
    "title": "1  General Idea of Machine Learning",
    "section": "1.3 The Limitations of the Techniques",
    "text": "1.3 The Limitations of the Techniques\nMachine learning is proven to help many organisations from different industries make better decisions based on insights from data such as the cases in detecting frauds in financial transactions or predicting cancer from medical x-rays. However, it is not a one-for-all solution as there are use cases when using machine learning will not help achieve the best result or is a waste of time. Careful consideration is taken place before actually applying the technique to the problem.\nIt is important for individuals or institutions to understand the limitations of the technique to decide whether it is the most appropriate approach. Some of the limitations of Machine learning include:\n\nIt usually needs a significant amount of data in order to learn effectively, especially if you want to train a deep learning model.\nThe performance of a model heavily depends on the data it is trained, meaning potential bias might be hard to detect. Overfitting is a common pitfall in machine learning where the model is to specialised in the training data and performs unreliably on test dataset.\nAs some techniques of machine learning, including the artificial neural network, is a black box system, the underlying insights behind the trained model can be hard, or even impossible, to understand. This is also a reason why some techniques are specifically designed for predictions, while others are used for inference."
  }
]